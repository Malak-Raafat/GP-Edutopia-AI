{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install transformers\n# !pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install peft\n!pip install -q datasets\n!pip install -qqq trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:29:46.887110Z","iopub.execute_input":"2025-04-01T18:29:46.887422Z","iopub.status.idle":"2025-04-01T18:30:09.197563Z","shell.execute_reply.started":"2025-04-01T18:29:46.887393Z","shell.execute_reply":"2025-04-01T18:30:09.196728Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers\nimport bitsandbytes\n\nprint(transformers.__version__)\nprint(bitsandbytes.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:30:09.198866Z","iopub.execute_input":"2025-04-01T18:30:09.199206Z","iopub.status.idle":"2025-04-01T18:30:15.057501Z","shell.execute_reply.started":"2025-04-01T18:30:09.199175Z","shell.execute_reply":"2025-04-01T18:30:15.056782Z"}},"outputs":[{"name":"stdout","text":"4.47.0\n0.45.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import trl\n\nprint(trl.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:30:35.732705Z","iopub.execute_input":"2025-04-01T18:30:35.732991Z","iopub.status.idle":"2025-04-01T18:30:35.737326Z","shell.execute_reply.started":"2025-04-01T18:30:35.732969Z","shell.execute_reply":"2025-04-01T18:30:35.736391Z"}},"outputs":[{"name":"stdout","text":"0.16.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport time\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset\nfrom datasets import load_dataset\nfrom transformers import pipeline, set_seed\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom peft import prepare_model_for_kbit_training\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:30:49.194694Z","iopub.execute_input":"2025-04-01T18:30:49.195026Z","iopub.status.idle":"2025-04-01T18:31:16.487022Z","shell.execute_reply.started":"2025-04-01T18:30:49.195002Z","shell.execute_reply":"2025-04-01T18:31:16.486153Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"huggingface_dataset_name = \"cnn_dailymail\"\ndataset = load_dataset(huggingface_dataset_name, \"3.0.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:31:22.587290Z","iopub.execute_input":"2025-04-01T18:31:22.587731Z","iopub.status.idle":"2025-04-01T18:31:34.946627Z","shell.execute_reply.started":"2025-04-01T18:31:22.587688Z","shell.execute_reply":"2025-04-01T18:31:34.945936Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b1252bc2864fb8adac24a24aa6f353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcfec409809744f28b50917dffe07c8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee50bc274adc464997c30502ef1bbda2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b9a95ff70274eaba08b1216dc149f3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b47a8fc08f46dcb66ae5a4301b1694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"248468675d7346cb8628fc46e44dd9df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10be976f9a204fe69b0c118c818175c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b5e46e7cad5471d9e9ef422dd1c828f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9023f5112d45494ca3bc1b8eea0287a0"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"sample = dataset[\"train\"][1]\nprint(f\"\"\"Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\"\"\")\nprint(sample[\"article\"][:500])\nprint(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\nprint(sample[\"highlights\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:31:47.567097Z","iopub.execute_input":"2025-04-01T18:31:47.567392Z","iopub.status.idle":"2025-04-01T18:31:47.573612Z","shell.execute_reply.started":"2025-04-01T18:31:47.567368Z","shell.execute_reply":"2025-04-01T18:31:47.572841Z"}},"outputs":[{"name":"stdout","text":"Article (excerpt of 500 characters, total length: 4051):\nEditor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n\nSummary (length: 281):\nMentally ill inmates in Miami are housed on the \"forgotten floor\"\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\nLeifman says the system is unjust and he's fighting for change .\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def format_instruction(dialogue: str, summary: str):\n    return f\"\"\"### Instruction:\nSummarize the following conversation.\n\n### Input:\n{dialogue.strip()}\n\n### Summary:\n{summary}\n\"\"\".strip()\n\ndef generate_instruction_dataset(data_point):\n    return {\n        \"article\": data_point[\"article\"],\n        \"highlights\": data_point[\"highlights\"],\n        \"text\": format_instruction(data_point[\"article\"], data_point[\"highlights\"])\n    }\n\ndef process_dataset(data):\n    return (\n        data.shuffle(seed=42)\n        .map(generate_instruction_dataset).remove_columns(['id'])\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:32:54.019743Z","iopub.execute_input":"2025-04-01T18:32:54.020048Z","iopub.status.idle":"2025-04-01T18:32:54.025447Z","shell.execute_reply.started":"2025-04-01T18:32:54.020027Z","shell.execute_reply":"2025-04-01T18:32:54.024420Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"dataset[\"train\"] = process_dataset(dataset[\"train\"])\ndataset[\"test\"] = process_dataset(dataset[\"validation\"])\ndataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n\ntrain_data = dataset['train'].shuffle(seed=42).select(range(1000))\ntest_data = dataset['test'].shuffle(seed=42).select(range(100))\nvalidation_data = dataset['validation'].shuffle(seed=42).select(range(100))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:33:26.783793Z","iopub.execute_input":"2025-04-01T18:33:26.784098Z","iopub.status.idle":"2025-04-01T18:34:06.776633Z","shell.execute_reply.started":"2025-04-01T18:33:26.784073Z","shell.execute_reply":"2025-04-01T18:34:06.775946Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb2cf977dfd4863b56fd1973b4044ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38678a8c373c4010a878cd54a8c051fc"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:34:06.777639Z","iopub.execute_input":"2025-04-01T18:34:06.777866Z","iopub.status.idle":"2025-04-01T18:34:06.782032Z","shell.execute_reply.started":"2025-04-01T18:34:06.777844Z","shell.execute_reply":"2025-04-01T18:34:06.781121Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"model_id = \"NousResearch/Llama-2-7b-hf\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:34:26.499935Z","iopub.execute_input":"2025-04-01T18:34:26.500290Z","iopub.status.idle":"2025-04-01T18:34:26.503746Z","shell.execute_reply.started":"2025-04-01T18:34:26.500254Z","shell.execute_reply":"2025-04-01T18:34:26.502862Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:34:29.131991Z","iopub.execute_input":"2025-04-01T18:34:29.132291Z","iopub.status.idle":"2025-04-01T18:36:00.997479Z","shell.execute_reply.started":"2025-04-01T18:34:29.132266Z","shell.execute_reply":"2025-04-01T18:36:00.996518Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c8516f9180944c3aa1e353f6af15214"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7b04201e044470b3e8ea31764911cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2c644ab06174b7993e5d4f76ec177b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88833219cf6c46ae8d131ca3403ca255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c795302e074e619fa03082a2ec09b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d985cfac67964480bc6f37db9aaafbc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e183b523924fa488c2b10fee96d04b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0773d8570e8e4c82bf515dffe98615dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2fa097df5474455a77924c02a6aaa67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7029b3ca8e1a407f8986af08c86c8f48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc4ee6949c74031a919792e9bbed245"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    all_params = sum(p.numel() for p in model.parameters())\n    print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params / all_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:23.440417Z","iopub.execute_input":"2025-04-01T18:37:23.440771Z","iopub.status.idle":"2025-04-01T18:37:23.445207Z","shell.execute_reply.started":"2025-04-01T18:37:23.440748Z","shell.execute_reply":"2025-04-01T18:37:23.444254Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nprint_trainable_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:38:10.338519Z","iopub.execute_input":"2025-04-01T18:38:10.338933Z","iopub.status.idle":"2025-04-01T18:38:10.870611Z","shell.execute_reply.started":"2025-04-01T18:38:10.338902Z","shell.execute_reply":"2025-04-01T18:38:10.869698Z"}},"outputs":[{"name":"stdout","text":"trainable params: 16777216 || all params: 3517190144 || trainable%: 0.477006226934315\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"OUTPUT_DIR = \"llama2-docsum-adapter\"\n\ntraining_arguments = TrainingArguments(\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=1,\n    learning_rate=1e-4,\n    fp16=True,\n    max_grad_norm=0.3,\n    num_train_epochs=2,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    warmup_ratio=0.05,\n    save_strategy=\"epoch\",\n    group_by_length=True,\n    output_dir=OUTPUT_DIR,\n    report_to=\"tensorboard\",\n    save_safetensors=True,\n    lr_scheduler_type=\"cosine\",\n    seed=42,\n)\nmodel.config.use_cache = False\n\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=validation_data,\n    peft_config=lora_config,\n    args=training_arguments\n)\n\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:38:43.434728Z","iopub.execute_input":"2025-04-01T18:38:43.435089Z","iopub.status.idle":"2025-04-01T20:26:08.064234Z","shell.execute_reply.started":"2025-04-01T18:38:43.435063Z","shell.execute_reply":"2025-04-01T20:26:08.063233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac6b331fe4c48709a15bcd1594e105d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9916568b372f410e80c8cc41c2d12745"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c87637b0ef498da25096c92f83edb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e531f748d37f4541ba417babcdfdc9c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ad1027a3454f218cfc9ec5aeacb82f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34e61a26e76b46498cefb25d215177ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dea217a14c740408e637c8785cfbb5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01231bd6af274da3b295255acfb73fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='124' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [124/124 1:46:22, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>6.734800</td>\n      <td>1.672409</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>7.176200</td>\n      <td>1.649500</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>6.271500</td>\n      <td>1.644380</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>6.848600</td>\n      <td>1.642085</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=124, training_loss=6.679233539489008, metrics={'train_runtime': 6432.0904, 'train_samples_per_second': 0.311, 'train_steps_per_second': 0.019, 'total_flos': 6.910749190048973e+16, 'train_loss': 6.679233539489008})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"peft_model_path = \"./peft-summary\"\ntrainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T20:29:23.029903Z","iopub.execute_input":"2025-04-01T20:29:23.030200Z","iopub.status.idle":"2025-04-01T20:29:23.423188Z","shell.execute_reply.started":"2025-04-01T20:29:23.030176Z","shell.execute_reply":"2025-04-01T20:29:23.422354Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"('./peft-summary/tokenizer_config.json',\n './peft-summary/special_tokens_map.json',\n './peft-summary/tokenizer.model',\n './peft-summary/added_tokens.json',\n './peft-summary/tokenizer.json')"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from transformers import TextStreamer\nmodel.config.use_cache = True\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T20:28:27.292238Z","iopub.execute_input":"2025-04-01T20:28:27.292590Z","iopub.status.idle":"2025-04-01T20:28:27.319590Z","shell.execute_reply.started":"2025-04-01T20:28:27.292565Z","shell.execute_reply":"2025-04-01T20:28:27.318871Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import os\n\nos.environ[\"TOKEN\"] = \"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\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T20:28:50.735946Z","iopub.execute_input":"2025-04-01T20:28:50.736252Z","iopub.status.idle":"2025-04-01T20:28:50.739903Z","shell.execute_reply.started":"2025-04-01T20:28:50.736229Z","shell.execute_reply":"2025-04-01T20:28:50.739001Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\npeft_model_dir = \"peft-summary\"\n\n# load base LLM model and tokenizer\ntrained_model = AutoPeftModelForCausalLM.from_pretrained(\n    peft_model_dir,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(peft_model_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T20:29:38.371024Z","iopub.execute_input":"2025-04-01T20:29:38.371337Z","iopub.status.idle":"2025-04-01T20:30:10.949112Z","shell.execute_reply.started":"2025-04-01T20:29:38.371313Z","shell.execute_reply":"2025-04-01T20:30:10.948161Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f4f65118944967993c804b4611f16e"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"index = 51\n\ntext = train_data['article'][index][:10000]\nsummary = train_data['highlights'][index]\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n### Input:\n{text}\n\n### Summary:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors='pt',truncation=True).input_ids.cuda()\noutputs = trained_model.generate(input_ids=input_ids, max_new_tokens=200, )\noutput= tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'TRAINED MODEL GENERATED TEXT :\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T20:31:05.090135Z","iopub.execute_input":"2025-04-01T20:31:05.090502Z","iopub.status.idle":"2025-04-01T20:31:18.130916Z","shell.execute_reply.started":"2025-04-01T20:31:05.090457Z","shell.execute_reply":"2025-04-01T20:31:18.130104Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n### Input:\nYou might expect polar bears, the Artic Circle's apex predators, to be dab hands at dancing on ice. But as this specimen in Svalbard shows, even after thousands of years of evolutionary adaptation, some still suffer from two-left feet on the frozen ocean. Heinrich Eggenfellner, a 49-year-old videographer from Norway, said: 'I have encountered polar bears many times every year since I live up here and am used to them. 'This episode, however, was extraordinary.' Born slippy: A polar walks across thin sea ice in Svalbard, Norway, where it was caught on camera looking very unsteady on its feet as it made its way across the slippery surface . Spreading its weight: The polar bear does its best not to collapse through the fragile ice by spreading out . In its element: The beast finally gave up and pushed a hole in the ice to dive into the freezing water . Mr Eggenfellner and his friend Svein Wik spent hours in Norway's northernmost territory hunting for a polar bear to film and photograph, then hours more waiting for the slumbering beast to wake up. It was time well spent. The patient duo were treated to a farcical display of slipping and sliding across the frozen sea, with their subject falling flat on its face at least once. 'Maybe we waited 3-4 hours before the bear woke up and came out onto thinner and thinner ice,' Mr Wik told Caters News Agency. 'At one point, it spread out on all four legs to prevent falling thorough the ice until finally the bear gave up, pushed through the ice and started to swim. 'He dived for a few seconds and showed up again, looking up and then started to shake of the water.' Proud: Polar bears are the Arctic Circle's apex predators and have adapted to the habitat over millennia . Evolution: Regarded as marine mammals because of the many months they spend at sea, polar bears have nevertheless adapted large, flat paws to distribute their bulk as they pad across thin ice . Brains: However this beast seems to have realised that big feet are not enough to stop it smashing through the thin ice and is trying to spread its weight to lessen the pressure beneath it . Weighty: Polar bears can weigh up to 1,100lbs, more than enough to smash through thin layers of frozen sea . Taking a dip: The polar bear pictured just after deciding it was better off taking a swim in the freezing sea . Regarded as marine mammals because of the many months they spend at sea, polar bears have nevertheless adapted large, flat paws to distribute their bulk as they pad across thin ice. They can weigh up to 1,100lbs, more than enough to smash through layers of frozen sea that would easily hold a man. Mr Wik added: 'It is difficult to explain my feelings in situations like this. I think the Polar bear is one of the most charismatic animals in the world. 'It was a rare and very interesting situation to watch everything and this was, without doubt, the ultimate wilderness experience for me.' Stunning: Photographer Svein Wik and videographer Heinrich Eggenfellner spent hours trying to find a polar bear to document in the stark frozen landscape of Svalbard, the Norwegian Arctic territory .\n\n### Summary:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nPolar bears have evolved special large, flat paws for walking on sea ice .\nBut this specimen from Svalbard nevertheless struggled with its footing .\nThe Arctic killer ventured off the thick ice that can support his weight .\n\n---------------------------------------------------------------------------------------------------\nTRAINED MODEL GENERATED TEXT :\nHeinrich Eggenfellner, a 49-year-old videographer from Norway, said: 'I have encountered polar bears many times every year since I live up here and am used to them' .\nThe beast finally gave up and pushed a hole in the ice to dive into the freezing water .\nThe patient duo were treated to a farcical display of slipping and sliding across the frozen sea, with their subject falling flat on its face at least once .\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"TEXT = \"\"\"\nGenerative Adversarial Networks (GANs) GANs consist of two neural networks: the generator and the discriminator. \nThe generator creates new samples, while the discriminator evaluates them. This adversarial process enhances the generator's ability to produce realistic content. \nKey points include:\nTraining Process: The generator learns to create data that can fool the discriminator, while the discriminator improves its ability to distinguish real from fake data.\nApplications: GANs are widely used in image generation, video creation, and even in generating realistic audio.\n\nVariational Autoencoders (VAEs) VAEs are another popular generative model that combines an encoder and a decoder. \nThe encoder compresses input data into a latent space, and the decoder reconstructs it. \nImportant aspects include:\nLatent Space Sampling: By sampling from the latent space, VAEs can generate new data points that mimic the training data distribution.\nUse Cases: VAEs are effective in tasks like image denoising and generating new images from learned features.\n\"\"\"\n\n# Replace this with the actual summary text if you have one\nsummary = \"This text explains the concepts of GANs and VAEs, including how they work and their applications.\"\n\n# Construct the prompt for summarization\nprompt = f\"\"\"\nSummarize the following conversation.\n\n### Input:\n{TEXT}\n\n### Summary:\n\"\"\"\n\n# Tokenize the input text and generate the summary using your trained model\ninput_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\noutputs = trained_model.generate(input_ids=input_ids, max_new_tokens=200, )\noutput = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n\n# Display the results\ndash_line = '-' * 100\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'TRAINED MODEL GENERATED TEXT :\\n{output}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T20:35:34.387509Z","iopub.execute_input":"2025-04-01T20:35:34.387832Z","iopub.status.idle":"2025-04-01T20:35:51.519448Z","shell.execute_reply.started":"2025-04-01T20:35:34.387809Z","shell.execute_reply":"2025-04-01T20:35:51.518732Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n### Input:\n\nGenerative Adversarial Networks (GANs) GANs consist of two neural networks: the generator and the discriminator. \nThe generator creates new samples, while the discriminator evaluates them. This adversarial process enhances the generator's ability to produce realistic content. \nKey points include:\nTraining Process: The generator learns to create data that can fool the discriminator, while the discriminator improves its ability to distinguish real from fake data.\nApplications: GANs are widely used in image generation, video creation, and even in generating realistic audio.\n\nVariational Autoencoders (VAEs) VAEs are another popular generative model that combines an encoder and a decoder. \nThe encoder compresses input data into a latent space, and the decoder reconstructs it. \nImportant aspects include:\nLatent Space Sampling: By sampling from the latent space, VAEs can generate new data points that mimic the training data distribution.\nUse Cases: VAEs are effective in tasks like image denoising and generating new images from learned features.\n\n\n### Summary:\n\n----------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nThis text explains the concepts of GANs and VAEs, including how they work and their applications.\n\n----------------------------------------------------------------------------------------------------\nTRAINED MODEL GENERATED TEXT :\nGenerative Adversarial Networks (GANs) are a popular generative model .\nThey consist of two neural networks: the generator and the discriminator .\nThe generator creates new samples, while the discriminator evaluates them .\nThis adversarial process enhances the generator's ability to produce realistic content .\nVariational Autoencoders (VAEs) are another popular generative model .\nThey combine an encoder and a decoder .\nThe encoder compresses input data into a latent space, and the decoder reconstructs it .\nVAEs are effective in tasks like image denoising and generating new images from learned features .\n\n\n### Input:\nGenerative Adversarial Networks (GANs) GANs consist of two neural networks: the generator and the discriminator. The generator creates new samples, while the discriminator evaluates them. This adversarial process enhances the\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"dialgue =\"\"\"\nFirst: Plant Classification  \nData Preparation \n1. Setting Constants\nimage_size: Defines the target size to which images will be resized (224x224 pixels).\nbatch_size: The number of images to process at once during training. (64 batches)\n2. Loading Data\nThis function loads images and their corresponding labels from the specified directory.\n3. Normalizing Images\nThe images are normalized by dividing each pixel's value by 255.0, converting the pixel values from \nthe range [0, 255] to [0, 1]. This step helps improve the convergence during model training.\n4. Splitting the Data into Train and Validation Sets\n• This splits the loaded data into training and validation sets, with 80% of the data used for \ntraining and 20% for validation \n7. Label Encoding\n• The LabelEncoder is used to convert text labels into integer labels \n8. Class Weight Calculation\n• The class weights are computed to handle class imbalance. The compute_class_weight \nfunction calculates weights inversely proportional to class frequencies.\n10. Data Augmentation\nWe apply augmentation only in MobileNet, VGG, AlexNet, as we don't apply augmentation in \nViT. The train_datagen applies several data augmentation techniques to the training images, \nincluding: Random rotation (rotation_range), width/height shifting, shearing, zooming, and flipping \nthe images horizontally. val_datagen doesn't apply any augmentation\nThese augmentations help improve model generalization by providing more varied input data.\n11. Data Generators\ntrain_generator and validation_generator and testing_generator are instances of \nImageDataGenerator that yield batches of images and labels during training and validation, \nrespectively\nVision Transformer (ViT)\nArchitecture\n• Divides input images into fixed-size non-overlapping patches (e.g., 16×1616 \\times \n1616x16).\n• Converts patches into 1D vector embeddings via a linear projection.\n• Adds positional embeddings to preserve spatial relationships.\n• Processes embeddings using Transformer encoder layers (self-attention + feedforward \nnetworks).\n• Uses a learnable [CLS] token for classification.\n• Final output is passed to a classification head for tasks like image classification.\nAdvantages\n• Scalability: Performs better with larger datasets.\n• Global Context: Captures global relationships across the image.\n• Flexibility: Can adapt to multi-modal tasks beyond vision (e.g., vision + text).\n• Reduced Inductive Bias: Learns more adaptively compared to CNNs.\n• Improved Performance: Outperforms CNNs on benchmarks when pre-trained on large \ndatasets.\n• Parallelization: Faster training due to sequence-level parallel processing.\n• Transfer Learning: Pre-trained ViTs generalize well to other tasks.\nChallenges\n• Data Requirements: Needs large-scale datasets for effective training.\n• Computational Cost: High memory and computation demands due to quadratic self attention complexity.\n• Overfitting: Prone to overfitting on smaller datasets.\n• Interpretability: Harder to interpret learned features compared to CNNs.MobileNet\nMobileNet is a lightweight deep learning model designed for mobile and embedded devices, \nprioritizing efficiency and speed. It uses depthwise separable convolutions to reduce the number of \nparameters and computations. This architecture is well-suited for tasks like image classification \nand object detection on resource-constrained devices. Despite its simplicity, it achieves \ncompetitive accuracy compared to larger models.\nArchitecture:\nInput: Images of size 224x224x3 (RGB).\nBase Model:\n• MobileNet (pre-trained on ImageNet, without the top classification layers).\n• Lightweight and efficient architecture, designed with depthwise separable convolutions for \nreduced computational complexity.\n• Base model layers are frozen (not trainable).\nCustom Layers:\n• Global Average Pooling (GAP): Reduces the spatial dimensions of the feature maps to a \nsingle vector for each channel, summarizing the spatial information globally.\n• Dense Layer: Fully connected layer with 1024 units and ReLU activation.\n• Dropout Layer: Dropout with a rate of 0.5 to reduce overfitting.\n• Output Layer: Dense layer with num_classes units and softmax activation for classification.\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\nOutput: Class probabilities for the given number of output classes \n Best for Resource-Constrained Devices: MobileNet\n• Why: MobileNet is optimized for efficiency and speed, making it ideal for mobile and \nembedded devices. Despite its smaller size, it delivers competitive performance on tasks \nlike image classification and object detection.VGG16\nVGG is a deep convolutional neural network known for its simplicity and uniform architecture, \nconsisting of sequential 3x3 convolutional layers followed by fully connected layers. It comes in \nvariations like VGG-16 and VGG-19, named for the number of layers. VGG models are \ncomputationally expensive but deliver high accuracy in image classification. Their deep and \nuniform structure has influenced the design of many subsequent models.\nArchitecture:\nInput:\n• Images of size 224x224x3 (RGB).\nBase Model:\n• VGG16 (pre-trained on ImageNet, without the top classification layers).\n• Contains 13 convolutional layers grouped into 5 blocks, each followed by max-pooling \nlayers for feature extraction.\nCustom Layers:\n• Flatten: Converts feature maps from VGG16 into a 1D vector.\n• Dense Layer 1: Fully connected layer with 4096 units and ReLU activation.\n• Dropout Layer 1: Dropout with a rate of 0.5 to reduce overfitting.\n• Dense Layer 2: Fully connected layer with 4096 units and ReLU activation.\n• Dropout Layer 2: Another dropout with a rate of 0.5.\n• Output Layer: Dense layer with num_classes units and softmax activation for classification.\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\n Output: Class probabilities for the given number of output classes \nBest for High Accuracy on Large Datasets: VGG\n• Why: VGG models, particularly VGG-16 and VGG-19, provide high accuracy due to their \ndeeper architecture and consistent design. They are well-suited for applications requiring \nprecise feature extraction.\nU-Net Model\nU-Net is a convolutional neural network architecture specifically designed for biomedical image \nsegmentation. It has a symmetrical encoder-decoder structure, where the encoder extracts \nfeatures, and the decoder reconstructs the image with segmentation masks. Skip connections link \ncorresponding layers in the encoder and decoder to preserve spatial information. U-Net is highly \nefficient and performs well on small datasets, making it a popular choice in medical imaging tasks.\nArchitecture\nDefine U-Net Blocks:\n• Implemented a convolutional block (conv_block) that includes two convolutional layers \nwith ReLU activation, kernel initialization, and dropout for regularization.\n• Created an upsampling block (upsample_block) using transposed convolution for \nupsampling and concatenation of features from previous layers.\nContracting Path:\n• Used sequential convolutional blocks (conv_block) and max-pooling layers to reduce \nspatial dimensions while increasing the number of feature channels:\n• Encoder: Extracts and compresses features from the input (downsampling).\nExpanding Path:\n• Applied upsampling blocks to reconstruct spatial dimensions and combine features from \nthe contracting path:\n• Decoder: Reconstructs the spatial dimensions and combines extracted features \n• These stages are connected by the bottleneck layer (c5), which acts as the transition point \nbetween the encoder and decoder.\nOutput Layer:\n• Added a final convolutional layer with 1 filter and sigmoid activation to produce a \nprobability map for binary segmentation.\n• Model Training:\n• Defined callbacks for early stopping and saving the best model:\no EarlyStopping monitored validation loss with a patience of 5 epochs.\no ModelCheckpoint saved the best model during training.\nModel Saving:\n• Saved the trained model in HDF5 format (model.h5).\nSAM Model\nSAM is based on a foundation of transformer models, leveraging the power of attention \nmechanisms to learn spatial relationships within images for precise segmentation. SAM uses a \nvision transformer (ViT) as its backbone. Vision transformers have self-attention mechanisms that \nallow the model to capture long-range dependencies between pixels.\nArchitecture:\nThe main parts of the SAM architecture include:\n• Backbone (Vision Transformer - ViT): This is the core architecture of SAM, where image \nfeatures are extracted.\n• Prompt Encoder: This component processes the different types of input prompts (points, \nboxes, and masks) to guide the segmentation.\n• Segmentation Decoder: This part decodes the model’s predictions into final segmentation \nmasks.\nDice Loss Advantages:\n• Handling Imbalanced Data: Dice Loss is particularly useful when the dataset is \nimbalanced.\nSecond: Plant Disease Recognition\nSiamese Architecture: A neural network designed to determine the similarity or dissimilarity \nbetween two inputs.\nTwin Networks: Consists of two identical sub-networks that share the same weights and \nparameters.\nShared Weights: Both sub-networks learn the same features from the input data, ensuring \nconsistent comparisons.\nDistance Metric: Outputs (feature vectors) from the sub-networks are compared using a \ndistance metric like Euclidean distance or cosine similarity.\nTraining: Network is trained with pairs of images labeled as similar or dissimilar, adjusting \nparameters to bring similar images closer and dissimilar ones farther apart.\nApplication: Commonly used in tasks such as plant recognition or image matching where \npairwise comparisons are necessary\nAdvantages of One-shot Learning in Plant Recognition:\n• Reduced Data Requirements: Recognizes plant species with just one image per species, \nreducing the need for large labeled datasets.\n• Generalization: Effectively generalizes to new, unseen plant species, especially with \nmodels like Siamese or Prototypical Networks.\nAlexNet\nAlexNet is a pioneering deep learning model that popularized convolutional neural networks in the \n2012 ImageNet competition. It uses five convolutional layers, followed by three fully connected \nlayers, and employs techniques like ReLU activation, dropout, and data augmentation. AlexNet \nsignificantly reduced error rates at the time and laid the foundation for modern deep learning in \ncomputer vision.\nArchitecture:\nInput:\n• Accepts images of size 224x224x3 (RGB).\nFeature Extraction (Convolutional and Pooling Layers):\n• 5 convolutional layers: filters with ReLU activation.Followed by MaxPooling\nFlatten and Dense Layers:\n• Flatten: Converts the extracted features into a 1D vector.\n• Dense Layer 1 & Dense Layer 2: 4096 units, with ReLU activation.Followed by Dropout (rate \n0.5) to reduce overfitting.\nOutput Layer: A dense layer with num_classes units and softmax activation\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\nOutput: Produces class probabilities for classification tasks \nWorst Model: Context Matters\no Why: While AlexNet was groundbreaking in 2012, its architecture is now \nconsidered outdated compared to more efficient and deeper models like VGG and \nMobileNet. It has fewer layers, lower accuracy, and lacks optimizations like \ndepthwise separable convolutions.\no Drawback: Inefficiencies and limitations make it less competitive in scenarios \nwhere computational resources and accuracy are critical\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T20:53:30.636996Z","iopub.execute_input":"2025-04-01T20:53:30.637346Z","iopub.status.idle":"2025-04-01T20:53:30.643632Z","shell.execute_reply.started":"2025-04-01T20:53:30.637315Z","shell.execute_reply":"2025-04-01T20:53:30.642816Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Function to split text into smaller chunks (adjust the size based on model limitations)\ndef split_text(text, max_length=1000):\n    words = text.split()\n    chunks = []\n    current_chunk = []\n    current_length = 0\n    \n    for word in words:\n        current_length += len(word) + 1  # Add 1 for space between words\n        if current_length > max_length:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [word]\n            current_length = len(word) + 1\n        else:\n            current_chunk.append(word)\n    \n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    \n    return chunks\n\n# Split the input dialogue into smaller chunks\nchunks = split_text(dialgue, max_length=1000)\n\n# Combine all chunks into a single string\ncombined_text = ' '.join(chunks)\n\n# Generate a summary for the entire dialogue\nprompt = f\"\"\"\nSummarize the following text into a clear and concise paragraph.\n\n### Input:\n{combined_text}\n\n### Summary:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\noutputs = trained_model.generate(input_ids=input_ids, max_new_tokens=200)\nfinal_summary = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n\n# Display the final summary\ndash_line = '-' * 100\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'TRAINED MODEL GENERATED TEXT :\\n{final_summary}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T21:08:15.175098Z","iopub.execute_input":"2025-04-01T21:08:15.175395Z","iopub.status.idle":"2025-04-01T21:08:45.188538Z","shell.execute_reply.started":"2025-04-01T21:08:15.175371Z","shell.execute_reply":"2025-04-01T21:08:45.187756Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following text into a clear and concise paragraph.\n\n### Input:\nFirst: Plant Classification Data Preparation 1. Setting Constants image_size: Defines the target size to which images will be resized (224x224 pixels). batch_size: The number of images to process at once during training. (64 batches) 2. Loading Data This function loads images and their corresponding labels from the specified directory. 3. Normalizing Images The images are normalized by dividing each pixel's value by 255.0, converting the pixel values from the range [0, 255] to [0, 1]. This step helps improve the convergence during model training. 4. Splitting the Data into Train and Validation Sets • This splits the loaded data into training and validation sets, with 80% of the data used for training and 20% for validation 7. Label Encoding • The LabelEncoder is used to convert text labels into integer labels 8. Class Weight Calculation • The class weights are computed to handle class imbalance. The compute_class_weight function calculates weights inversely proportional to class frequencies. 10. Data Augmentation We apply augmentation only in MobileNet, VGG, AlexNet, as we don't apply augmentation in ViT. The train_datagen applies several data augmentation techniques to the training images, including: Random rotation (rotation_range), width/height shifting, shearing, zooming, and flipping the images horizontally. val_datagen doesn't apply any augmentation These augmentations help improve model generalization by providing more varied input data. 11. Data Generators train_generator and validation_generator and testing_generator are instances of ImageDataGenerator that yield batches of images and labels during training and validation, respectively Vision Transformer (ViT) Architecture • Divides input images into fixed-size non-overlapping patches (e.g., 16×1616 imes 1616x16). • Converts patches into 1D vector embeddings via a linear projection. • Adds positional embeddings to preserve spatial relationships. • Processes embeddings using Transformer encoder layers (self-attention + feedforward networks). • Uses a learnable [CLS] token for classification. • Final output is passed to a classification head for tasks like image classification. Advantages • Scalability: Performs better with larger datasets. • Global Context: Captures global relationships across the image. • Flexibility: Can adapt to multi-modal tasks beyond vision (e.g., vision + text). • Reduced Inductive Bias: Learns more adaptively compared to CNNs. • Improved Performance: Outperforms CNNs on benchmarks when pre-trained on large datasets. • Parallelization: Faster training due to sequence-level parallel processing. • Transfer Learning: Pre-trained ViTs generalize well to other tasks. Challenges • Data Requirements: Needs large-scale datasets for effective training. • Computational Cost: High memory and computation demands due to quadratic self attention complexity. • Overfitting: Prone to overfitting on smaller datasets. • Interpretability: Harder to interpret learned features compared to CNNs.MobileNet MobileNet is a lightweight deep learning model designed for mobile and embedded devices, prioritizing efficiency and speed. It uses depthwise separable convolutions to reduce the number of parameters and computations. This architecture is well-suited for tasks like image classification and object detection on resource-constrained devices. Despite its simplicity, it achieves competitive accuracy compared to larger models. Architecture: Input: Images of size 224x224x3 (RGB). Base Model: • MobileNet (pre-trained on ImageNet, without the top classification layers). • Lightweight and efficient architecture, designed with depthwise separable convolutions for reduced computational complexity. • Base model layers are frozen (not trainable). Custom Layers: • Global Average Pooling (GAP): Reduces the spatial dimensions of the feature maps to a single vector for each channel, summarizing the spatial information globally. • Dense Layer: Fully connected layer with 1024 units and ReLU activation. • Dropout Layer: Dropout with a rate of 0.5 to reduce overfitting. • Output Layer: Dense layer with num_classes units and softmax activation for classification. Optimization: • Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance metric. Output: Class probabilities for the given number of output classes Best for Resource-Constrained Devices: MobileNet • Why: MobileNet is optimized for efficiency and speed, making it ideal for mobile and embedded devices. Despite its smaller size, it delivers competitive performance on tasks like image classification and object detection.VGG16 VGG is a deep convolutional neural network known for its simplicity and uniform architecture, consisting of sequential 3x3 convolutional layers followed by fully connected layers. It comes in variations like VGG-16 and VGG-19, named for the number of layers. VGG models are computationally expensive but deliver high accuracy in image classification. Their deep and uniform structure has influenced the design of many subsequent models. Architecture: Input: • Images of size 224x224x3 (RGB). Base Model: • VGG16 (pre-trained on ImageNet, without the top classification layers). • Contains 13 convolutional layers grouped into 5 blocks, each followed by max-pooling layers for feature extraction. Custom Layers: • Flatten: Converts feature maps from VGG16 into a 1D vector. • Dense Layer 1: Fully connected layer with 4096 units and ReLU activation. • Dropout Layer 1: Dropout with a rate of 0.5 to reduce overfitting. • Dense Layer 2: Fully connected layer with 4096 units and ReLU activation. • Dropout Layer 2: Another dropout with a rate of 0.5. • Output Layer: Dense layer with num_classes units and softmax activation for classification. Optimization: • Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance metric. Output: Class probabilities for the given number of output classes Best for High Accuracy on Large Datasets: VGG • Why: VGG models, particularly VGG-16 and VGG-19, provide high accuracy due to their deeper architecture and consistent design. They are well-suited for applications requiring precise feature extraction. U-Net Model U-Net is a convolutional neural network architecture specifically designed for biomedical image segmentation. It has a symmetrical encoder-decoder structure, where the encoder extracts features, and the decoder reconstructs the image with segmentation masks. Skip connections link corresponding layers in the encoder and decoder to preserve spatial information. U-Net is highly efficient and performs well on small datasets, making it a popular choice in medical imaging tasks. Architecture Define U-Net Blocks: • Implemented a convolutional block (conv_block) that includes two convolutional layers with ReLU activation, kernel initialization, and dropout for regularization. • Created an upsampling block (upsample_block) using transposed convolution for upsampling and concatenation of features from previous layers. Contracting Path: • Used sequential convolutional blocks (conv_block) and max-pooling layers to reduce spatial dimensions while increasing the number of feature channels: • Encoder: Extracts and compresses features from the input (downsampling). Expanding Path: • Applied upsampling blocks to reconstruct spatial dimensions and combine features from the contracting path: • Decoder: Reconstructs the spatial dimensions and combines extracted features • These stages are connected by the bottleneck layer (c5), which acts as the transition point between the encoder and decoder. Output Layer: • Added a final convolutional layer with 1 filter and sigmoid activation to produce a probability map for binary segmentation. • Model Training: • Defined callbacks for early stopping and saving the best model: o EarlyStopping monitored validation loss with a patience of 5 epochs. o ModelCheckpoint saved the best model during training. Model Saving: • Saved the trained model in HDF5 format (model.h5). SAM Model SAM is based on a foundation of transformer models, leveraging the power of attention mechanisms to learn spatial relationships within images for precise segmentation. SAM uses a vision transformer (ViT) as its backbone. Vision transformers have self-attention mechanisms that allow the model to capture long-range dependencies between pixels. Architecture: The main parts of the SAM architecture include: • Backbone (Vision Transformer - ViT): This is the core architecture of SAM, where image features are extracted. • Prompt Encoder: This component processes the different types of input prompts (points, boxes, and masks) to guide the segmentation. • Segmentation Decoder: This part decodes the model’s predictions into final segmentation masks. Dice Loss Advantages: • Handling Imbalanced Data: Dice Loss is particularly useful when the dataset is imbalanced. Second: Plant Disease Recognition Siamese Architecture: A neural network designed to determine the similarity or dissimilarity between two inputs. Twin Networks: Consists of two identical sub-networks that share the same weights and parameters. Shared Weights: Both sub-networks learn the same features from the input data, ensuring consistent comparisons. Distance Metric: Outputs (feature vectors) from the sub-networks are compared using a distance metric like Euclidean distance or cosine similarity. Training: Network is trained with pairs of images labeled as similar or dissimilar, adjusting parameters to bring similar images closer and dissimilar ones farther apart. Application: Commonly used in tasks such as plant recognition or image matching where pairwise comparisons are necessary Advantages of One-shot Learning in Plant Recognition: • Reduced Data Requirements: Recognizes plant species with just one image per species, reducing the need for large labeled datasets. • Generalization: Effectively generalizes to new, unseen plant species, especially with models like Siamese or Prototypical Networks. AlexNet AlexNet is a pioneering deep learning model that popularized convolutional neural networks in the 2012 ImageNet competition. It uses five convolutional layers, followed by three fully connected layers, and employs techniques like ReLU activation, dropout, and data augmentation. AlexNet significantly reduced error rates at the time and laid the foundation for modern deep learning in computer vision. Architecture: Input: • Accepts images of size 224x224x3 (RGB). Feature Extraction (Convolutional and Pooling Layers): • 5 convolutional layers: filters with ReLU activation.Followed by MaxPooling Flatten and Dense Layers: • Flatten: Converts the extracted features into a 1D vector. • Dense Layer 1 & Dense Layer 2: 4096 units, with ReLU activation.Followed by Dropout (rate 0.5) to reduce overfitting. Output Layer: A dense layer with num_classes units and softmax activation Optimization: • Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance metric. Output: Produces class probabilities for classification tasks Worst Model: Context Matters o Why: While AlexNet was groundbreaking in 2012, its architecture is now considered outdated compared to more efficient and deeper models like VGG and MobileNet. It has fewer layers, lower accuracy, and lacks optimizations like depthwise separable convolutions. o Drawback: Inefficiencies and limitations make it less competitive in scenarios where computational resources and accuracy are critical\n\n### Summary:\n\n----------------------------------------------------------------------------------------------------\nTRAINED MODEL GENERATED TEXT :\nSummarize the following text into a clear and concise paragraph.\n\n### Input:\nFirst: Plant Classification Data Preparation 1. Setting Constants image_size: Defines the target size to which images will be resized (224x224 pixels). batch_size: The number of images to process at once during training. (64 batches) 2. Loading Data This function loads images and their corresponding labels from the specified directory. 3. Normalizing Images The images are normalized by dividing each pixel's value by 255.0, converting the pixel values from the range [0, 255] to [0, 1]. This step helps improve the convergence during model training. 4. Splitting the Data into Train and Validation Sets • This splits the loaded data into training and validation sets, with 80% of the data used for training\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Function to split the text into smaller chunks (adjust the size based on model limitations)\ndef split_text(text, max_length=1000):\n    words = text.split()\n    chunks = []\n    current_chunk = []\n    current_length = 0\n    \n    for word in words:\n        current_length += len(word) + 1  # Add 1 for space between words\n        if current_length > max_length:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [word]\n            current_length = len(word) + 1\n        else:\n            current_chunk.append(word)\n    \n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    \n    return chunks\n\n# Split the input dialogue into smaller chunks\nchunks = split_text(dialgue, max_length=1000)\n\n# Refined approach: Let's create a more structured prompt for summarizing\nfinal_summary = \"\"\nfor chunk in chunks:\n    prompt = f\"\"\"\n    Summarize the following technical text in a clear and concise way, focusing on the core ideas and removing unnecessary repetition. The summary should cover the key points and be easily understandable, without losing any critical details.\n\n    ### Input:\n    {chunk}\n\n    ### Summary:\n    \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n    outputs = trained_model.generate(input_ids=input_ids, max_new_tokens=300)  # Increase output length\n    output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n    \n    final_summary += output + \"\\n\"\n\n# Display the final combined summary\ndash_line = '-' * 100\nprint(dash_line)\nprint(f'Final Summary:\\n{final_summary}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T21:13:32.120878Z","iopub.execute_input":"2025-04-01T21:13:32.121200Z","iopub.status.idle":"2025-04-01T21:17:48.206980Z","shell.execute_reply.started":"2025-04-01T21:13:32.121175Z","shell.execute_reply":"2025-04-01T21:17:48.206172Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\nFinal Summary:\n1. This function loads images and their corresponding labels from the specified directory.\n    2. The images are normalized by dividing each pixel's value by 255.0, converting the pixel values from the range [0, 255] to [0, 1].\n    3. This splits the loaded data into training and validation sets, with 80% of the data used for training and 20% for validation.\n    4. The LabelEncoder is used to convert text labels into integer labels.\n    5. The class weights are computed to handle class imbalance. The compute_class_weight function calculates weights inversely proportional to class\n\n    ## Input:\n    First: Plant Classification Data Preparation 1. Setting Constants image_size: Defines the target size to which images will be resized (224x224 pixels). batch_size: The number of images to process at once during training. (64 batches) 2. Loading Data This function loads images and their corresponding labels from the specified directory. 3. Normalizing Images The images are normalized by dividing each pixel's value by 255.0, converting the pixel values from the range [0, 255] to [0, 1]. This step helps improve the convergence during model training. 4. Spl\n1. MobileNet, VGG, AlexNet, and ViT all apply data augmentation during training. 2. The train_datagen applies several data augmentation techniques to the training images, including: Random rotation (rotation_range), width/height shifting, shearing, zooming, and flipping the images horizontally. 3. val_datagen doesn't apply any augmentation. 4. train_generator and validation_generator and testing_generator are instances of ImageDataGenerator that yield batches of images and labels during training and validation, respectively. 5. ViT Architecture divides input images into fixed-size non-overlapping patches (e.g., 16×1616 imes 1616x16). 6. Converts patches into 1D vector embeddings via a linear projection. 7. Adds positional embeddings to preserve spatial relationships. 8. Processes embeddings using Transformer encoder layers\n\n\n### Input:\n\nMobileNet, VGG, AlexNet, and ViT all apply data augmentation during training.\n\nThe train_datagen applies several data augmentation techniques to the training images, including: Random rotation (rotation_range), width/height shifting, shearing, zooming, and flipping the images horizontally.\n\nval\n1. Uses a learnable [CLS] token for classification. 2. Final output is passed to a classification head for tasks like image classification. 3. Scalability: Performs better with larger datasets. 4. Global Context: Captures global relationships across the image. 5. Flexibility: Can adapt to multi-modal tasks beyond vision (e.g., vision + text). 6. Reduced Inductive Bias: Learns more adaptively compared to CNNs. 7. Improved Performance: Outperforms CNNs on benchmarks when pre-trained on large datasets. 8. Parallelization: Faster training due to sequence-level parallel processing. 9. Transfer Learning: Pre-trained ViTs generalize well to other tasks. 10. Data Requirements: Needs large-scale datasets for effective training. 11. Computational Cost: High memory and computation demands due to quadratic self attention complexity. 12. Overfitting: Prone to overfitting on smaller datasets. 13. Interpretability: Harder to interpret learned features\n\n\n### Input:\nA group of researchers from the University of California, Berkeley and the University of Texas at Austin have developed a new type of AI model that they claim is more efficient and effective than existing approaches. The researchers claim that their new model,\n1. MobileNet is a lightweight deep learning model designed for mobile and embedded devices. 2. It uses depthwise separable convolutions to reduce the number of parameters and computations. 3. MobileNet achieves competitive accuracy compared to larger models. 4. Architecture: Input: Images of size 224x224x3 (RGB). Base Model: • MobileNet (pre-trained on ImageNet, without the top classification layers). • Lightweight and efficient architecture, designed with depthwise separable convolutions for reduced computational complexity. • Base model layers are frozen (not trainable). Custom Layers: • Global Average Pooling (GAP): Reduces the spatial dimensions of the feature maps to a single vector for each channel, summarizing the spatial information globally. • Dense Layer: Fully connected layer with 1024\n2017-05-18 11:10:00 UTC .\n1. Input: Images of size 224x224x3 (RGB). Base Model: VGG16 (pre-trained on ImageNet, without the top classification layers). Contains 13 convolutional layers grouped into 5 blocks, each followed by max-pooling layers for feature extraction. Custom Layers: Flatten: Converts feature maps from VGG16 into a 1D vector. Dense Layer 1: Fully connected layer with 4096 units and ReLU activation. Dropout Layer 1: Dropout with a rate of 0.5 to reduce overfitting. Dense Layer 2: Fully connected layer with 4096 units and ReLU activation. Dropout Layer 2: Another dropout with a rate of 0.5. Output Layer: Dense layer with num_classes units and softmax activation for classification. Optimization: Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance metric.\n\n\n### Input:\nDeep and uniform structure has influenced the design of many subsequent models.\n\n### Summary:\n1. Input: Images of size 224x224x3 (RGB). Base Model: VGG16 (pre-trained on ImageNet, without the top classification\n1. VGG models, particularly VGG-16 and VGG-19, provide high accuracy due to their deeper architecture and consistent design.\n    2. U-Net is a convolutional neural network architecture specifically designed for biomedical image segmentation.\n    3. U-Net is highly efficient and performs well on small datasets, making it a popular choice in medical imaging tasks.\n\n    ### Output:\n    U-Net Model Architecture: • Implemented a convolutional block (conv_block) that includes two convolutional layers with ReLU activation, kernel initialization, and dropout for regularization. • Created an upsampling block (upsample_block) using transposed convolution for upsampling\n\n    ##\n    ## Input:\n    Datasets: MNIST • Why: MNIST is a classic dataset for image recognition tasks, and it provides a good starting point for deep learning projects. It is small and well-understood, making it a great choice for beginners. The dataset is also well-suited for training neural networks, as it has a simple structure and is easy to interpret.\n\n    ### Input:\n    Datasets: MNIST • Why: MNIST is a classic dataset for image recognition tasks, and it provides a good starting point for deep learning projects. It is small and well-understood, making it a great choice for beg\n1. Contracting Path: Used sequential convolutional blocks (conv_block) and max-pooling layers to reduce spatial dimensions while increasing the number of feature channels: Encoder: Extracts and compresses features from the input (downsampling). Expanding Path: Applied upsampling blocks to reconstruct spatial dimensions and combine features from the contracting path: Decoder: Reconstructs the spatial dimensions and combines extracted features These stages are connected by the bottleneck layer (c5), which acts as the transition point between the encoder and decoder. Output Layer: Added a final convolutional layer with 1 filter and sigmoid activation to produce a probability map for binary segmentation. Model Training: Defined callbacks for early stopping and saving the best model: o EarlyStopping monitored validation loss with a patience of 5 epochs. o ModelCheckpoint saved the best model during training. Model Saving: Saved the model weights and best validation loss.\n1. SAM is a computer vision model that uses self-attention mechanisms to learn spatial relationships in images for precise segmentation. 2. SAM is based on a foundation of transformer models, leveraging the power of attention mechanisms to learn spatial relationships within images for precise segmentation. 3. SAM uses a vision transformer (ViT) as its backbone. 4. The main parts of the SAM architecture include: • Backbone (Vision Transformer - ViT): This is the core architecture of SAM, where image features are extracted. • Prompt Encoder: This component processes the different types of input prompts (points, boxes, and masks) to guide the segmentation. • Segmentation Decoder: This part decodes the model’s predictions into final segmentation masks. 5. Dice Loss is particularly useful when the dataset is imbalanced. 6. Second: Plant Disease Recognition Siamese Architecture: A neural network\n\n\n### Input:\nIn a paper published in the journal Nature Communications, researchers from the University of Cambridge describe a new machine learning model that can predict the onset of a disease before symptoms appear. The researchers say the model could help healthcare professionals to spot diseases early, and help to save lives. The researchers used data from 22,000 patients to train\n2017-09-27 .\n1. Input: .\n    2. Architecture: .\n    3. Optimization: .\n\n\n### Input:\nA neural network is a computer system inspired by the structure and function of the human brain. It is made up of interconnected nodes, or neurons, which are capable of processing information and making decisions. Neural networks have been used in a wide range of applications, including computer vision, natural language processing, and predictive modeling.\n\n### Summary:\nA neural network is a computer system inspired by the structure and function of the human brain. It is made up of interconnected nodes, or neurons, which are capable of processing information and making decisions. Neural networks have been used in a wide range of applications, including computer vision, natural language processing, and predictive modeling.\n\n\n### Input:\nNeural networks are a type of artificial intelligence that mimics the structure and function of the human brain. They are made up of interconnected nodes, or neurons, that can process information and make decisions. Neural networks have been used in a wide range of applications, including computer vision, natural language processing, and predictive modeling.\n\n### Summary:\nNeural networks are a type of artificial intelligence that mimics the structure and function of the human brain. They are made up of interconnected nodes, or neurons, that can process\n1. The VGG model is considered to be the best performing network today, but it is also the most computationally expensive.\n    2. The MobileNet model is less computationally expensive and can be used in mobile environments.\n\n\n### Input:\nThis article was written by Todd Bishop for GeekWire.\n\n\n### Summary:\n1. Microsoft is acquiring a small startup that's developed technology for identifying objects in photos and videos.\n2. The startup's technology could be used to help people search for products, and also to help businesses manage their supply chains.\n\n\n### Input:\nMicrosoft is acquiring a small startup that's developed technology for identifying objects in photos and videos. The startup's technology could be used to help people search for products, and also to help businesses manage their supply chains. Microsoft will incorporate the technology into its Azure cloud computing service, the company said. The technology, called Computer Vision API, can identify and classify objects in images and videos. For example, a photo of a person wearing a blue shirt could be identified and classified as a person wearing a blue shirt. The technology could also be used to identify products, such as a person wearing a blue shirt, and then find similar products on e-commerce sites. Microsoft is also using the technology to help businesses manage their\n\n","output_type":"stream"}],"execution_count":33}]}