{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install required packages\n!pip install -q datasets tqdm huggingface-hub groq python-dotenv\n!pip install -q --upgrade huggingface-hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport time\nfrom tqdm import tqdm\nfrom datasets import load_dataset, Dataset\nfrom dotenv import load_dotenv\nfrom groq import Groq\n\n# Initialize multiple Groq clients with API key rotation\nfrom kaggle_secrets import UserSecretsClient\n\n# List of 10 API keys (replace with your actual secret names)\nAPI_KEY_NAMES = [\n    \"GROQ_API_KEY_8\", \"GROQ_API_KEY_2\"\n]\n\n# Create clients for all API keys\nclients = []\nuser_secrets = UserSecretsClient()\nfor key_name in API_KEY_NAMES:\n    try:\n        api_key = user_secrets.get_secret(key_name)\n        if api_key:\n            clients.append(Groq(api_key=api_key))\n    except:\n        continue\n\nif not clients:\n    raise ValueError(\"No valid Groq API keys found!\")\n\ncurrent_client_index = 0\n\ndef get_client():\n    \"\"\"Rotate through available clients\"\"\"\n    global current_client_index\n    client = clients[current_client_index]\n    current_client_index = (current_client_index + 1) % len(clients)\n    return client\n\n# Disable file locking for Kaggle\nos.environ[\"HF_DATASETS_DISABLE_FILE_LOCKING\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Your exact summarizer function\ndef create_educational_summary(content):\n    \"\"\"Generate educational summaries with your exact specifications\"\"\"\n    system_message = \"\"\"ou are an expert educational content summarizer with expertise across ALL academic disciplines. Your task is to create a comprehensive yet concise summary that captures ALL important information while removing redundancy.\n\nFollow these guidelines:\n1. Capture EVERY new term, concept, or definition introduced\n2. Preserve ALL explanations of new concepts or ideas\n3. Maintain ALL key relationships between concepts\n4. Include ALL practical applications and examples\n5. Remove ONLY redundant explanations and repeated information\n6. Use clear, direct language without unnecessary elaboration\n7. Adapt your summary format to the specific field of study\n\nFor ANY scientific content:\n- ALWAYS include ALL mathematical equations, formulas, and rules with their explanations\n- ALWAYS preserve the exact notation and symbols used in the original content\n- ALWAYS explain what each variable and symbol represents\n- ALWAYS include any important constants or parameters\n- ALWAYS explain the context and application of each equation\n\nFor ANY humanities content:\n- Include literary analysis, themes, and symbolism\n- Capture historical context, timelines, and cause-and-effect relationships\n- Preserve philosophical arguments and ethical considerations\n- Note cultural significance and societal impact\n\nFor ANY social sciences:\n- Include theoretical frameworks and methodologies\n- Capture statistical data and research findings\n- Highlight policy implications and real-world applications\n- Note cultural and societal contexts\n\nFor ANY arts and languages:\n- Include techniques, styles, and movements\n- Capture grammar rules, vocabulary, and cultural context\n- Highlight artistic elements and creative processes\n- Note historical development and influence\n\nFormat your response with these sections (include only those relevant to the content):\n- New Terms and Definitions (list ALL new terms with their explanations)\n- Core Concepts (include ALL key concepts with their explanations)\n- [Field-Specific Content] (e.g., Mathematical Content, Literary Analysis, Historical Context)\n- Relationships and Connections (how concepts relate to each other)\n- Applications and Examples (ALL practical uses and examples)\n\nImportant: Do not omit any new concept, definition, or explanation. Only remove truly redundant content. Adapt your format to best represent the specific field of study. For ANY scientific content, ALWAYS include ALL equations and formulas with their explanations.\"\"\"\n\n    for attempt in range(5):  # 5 retry attempts\n        try:\n            client = get_client()  # Get client with rotation\n            response = client.chat.completions.create(\n                model=\"llama-3.2-90b-vision-preview\",\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": content}  # No truncation\n                ],\n                temperature=0.2,\n                timeout=60\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            if attempt < 4:\n                wait_time = 2 ** (attempt + 3)  # Exponential backoff: 8,16,32,64s\n                print(f\"Attempt {attempt+1} failed. Waiting {wait_time}s...\")\n                time.sleep(wait_time)\n                continue\n            return f\"Error after 5 attempts: {str(e)}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Load exactly 1000 samples (no truncation)\ndef load_samples(dataset_ranges=None):\n    \"\"\"\n    Load samples across all datasets with customizable ranges for each dataset\n    \n    Args:\n        dataset_ranges: Dictionary specifying ranges for each dataset\n                        Format: {'lectures': (start, end), \n                                'education': (start, end),\n                                'feynman': (start, end)}\n                        If None, uses default ranges to get exactly 1000 samples\n    \"\"\"\n    samples = []\n    \n    # Default ranges that give exactly 1000 samples (334 + 333 + 333)\n    default_ranges = {\n        'lectures': (0, 333),\n        'education': (0, 333),\n        'feynman': (0, 334)\n    }\n    \n    ranges = dataset_ranges if dataset_ranges else default_ranges\n    \n    # Dataset 1: AlexanderBenady/generated_lectures\n    try:\n        ds1 = load_dataset(\"AlexanderBenady/generated_lectures\", split=\"train\")\n        start, end = ranges['lectures']\n        samples.extend([{\"text\": x['Lecture'], \"source\": \"lectures\"} \n                       for x in ds1.select(range(start, end))])\n        print(f\"Loaded {end-start} samples from lectures dataset (indices {start}-{end})\")\n    except Exception as e:\n        print(f\"Error loading lectures: {str(e)}\")\n    \n    # Dataset 2: ajibawa-2023/Education-High-School-Students\n    try:\n        ds2 = load_dataset(\"ajibawa-2023/Education-High-School-Students\", split=\"train\")\n        start, end = ranges['education']\n        samples.extend([{\"text\": x['text'], \"source\": \"education\"} \n                       for x in ds2.select(range(start, end))])\n        print(f\"Loaded {end-start} samples from education dataset (indices {start}-{end})\")\n    except Exception as e:\n        print(f\"Error loading education: {str(e)}\")\n    \n    # Dataset 3: enesxgrahovac/the-feynman-lectures-on-physics\n    try:\n        ds3 = load_dataset(\"enesxgrahovac/the-feynman-lectures-on-physics\", split=\"train\")\n        start, end = ranges['feynman']\n        samples.extend([{\"text\": x['section_text'], \"source\": \"feynman\"} \n                       for x in ds3.select(range(start, end))])\n        print(f\"Loaded {end-start} samples from feynman dataset (indices {start}-{end})\")\n    except Exception as e:\n        print(f\"Error loading feynman: {str(e)}\")\n    \n    return samples","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_and_create_hf_dataset():\n    # 1. Load samples\n    samples = load_samples()\n    if not samples:\n        raise ValueError(\"No samples loaded - check dataset errors\")\n    \n    # 2. Process with summarizer\n    results = []\n    for i, sample in enumerate(tqdm(samples, desc=\"Processing\")):\n        try:\n            # Call your summarizer (no truncation)\n            summary = create_educational_summary(sample[\"text\"])\n            \n            results.append({\n                \"original_text\": sample[\"text\"],\n                \"summary\": summary,\n                \"source\": sample[\"source\"],\n                \"sample_id\": i\n            })\n            \n            # Save progress every 25 samples\n            if (i + 1) % 25 == 0:\n                with open('progress_checkpoint.json', 'w') as f:\n                    json.dump(results, f, indent=2)\n                print(f\"\\nCheckpoint saved: {len(results)} processed\")\n            \n            # Dynamic delay based on recent errors\n            delay = 10 if results and \"Error\" in results[-1][\"summary\"] else 5\n            time.sleep(delay)\n            \n        except Exception as e:\n            print(f\"\\nCritical error on sample {i}: {str(e)}\")\n            continue\n    \n    # 3. Create Hugging Face Dataset\n    hf_dataset = Dataset.from_dict({\n        \"original_text\": [x[\"original_text\"] for x in results],\n        \"summary\": [x[\"summary\"] for x in results],\n        \"source\": [x[\"source\"] for x in results],\n        \"sample_id\": [x[\"sample_id\"] for x in results]\n    })\n    \n    # 4. Save dataset\n    hf_dataset.save_to_disk(\"education_summaries_dataset\")\n    \n    # 5. Create dataset card\n    with open(\"education_summaries_dataset/README.md\", \"w\") as f:\n        f.write(\"# Education Summaries Dataset\\n\\n\")\n        f.write(\"## Dataset Summary\\n\")\n        f.write(f\"- Total samples: {len(results)}\\n\")\n        f.write(\"- Sources: education, feynman\\n\")\n        f.write(\"- Generated using Groq's llama3-70b-8192 model\\n\\n\")\n        f.write(\"## Fields\\n\")\n        f.write(\"- original_text: Full original educational content\\n\")\n        f.write(\"- summary: AI-generated comprehensive summary\\n\")\n        f.write(\"- source: Dataset origin\\n\")\n        f.write(\"- sample_id: Unique identifier\\n\")\n    \n    print(\"\\n\" + \"=\"*50)\n    print(f\"Completed processing {len(results)} samples\")\n    print(\"Hugging Face dataset saved to: education_summaries_dataset\")\n    print(\"=\"*50)\n    \n    return hf_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Run the complete pipeline\n    dataset = process_and_create_hf_dataset()\n    \n    # Show sample output\n    print(\"\\nSample from final dataset:\")\n    sample = dataset[0]\n    print(f\"Source: {sample['source']}\")\n    print(f\"Original length: {len(sample['original_text'])} chars\")\n    print(f\"Summary length: {len(sample['summary'])} chars\")\n    print(\"\\nSummary preview:\")\n    print(sample['summary'][:200] + \"...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}