{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers datasets evaluate accelerate\n!pip install scikit-learn\n!pip install tensorboard","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:56:39.360449Z","iopub.execute_input":"2025-03-26T18:56:39.360783Z","iopub.status.idle":"2025-03-26T18:57:02.659533Z","shell.execute_reply.started":"2025-03-26T18:56:39.360759Z","shell.execute_reply":"2025-03-26T18:57:02.658610Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers\n  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting datasets\n  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting accelerate\n  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.50.1-py3-none-any.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: datasets, transformers, evaluate, accelerate\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.3.1\n    Uninstalling datasets-3.3.1:\n      Successfully uninstalled datasets-3.3.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.2.1\n    Uninstalling accelerate-1.2.1:\n      Successfully uninstalled accelerate-1.2.1\nSuccessfully installed accelerate-1.5.2 datasets-3.4.1 evaluate-0.4.3 transformers-4.50.1\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.68.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\nRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (24.2)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.12.0->tensorboard) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -U transformers\n!pip install -U datasets\n!pip install tensorboard\n!pip install sentencepiece\n!pip install accelerate\n!pip install evaluate\n!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:59:16.235040Z","iopub.execute_input":"2025-03-26T18:59:16.235502Z","iopub.status.idle":"2025-03-26T18:59:45.149424Z","shell.execute_reply.started":"2025-03-26T18:59:16.235457Z","shell.execute_reply":"2025-03-26T18:59:45.148615Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.50.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.68.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\nRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (24.2)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.0->tensorboard) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.12.0->tensorboard) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.5.2)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=278e3217ea8803646b7d0f667d27e9161c5659183fc6a04063272b5070d659fb\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport pprint\nimport evaluate\nimport numpy as np\n \nfrom transformers import (\n    T5Tokenizer,\n    T5ForConditionalGeneration,\n    TrainingArguments,\n    Trainer\n)\nfrom datasets import load_dataset\ndataset = load_dataset('gopalkalpande/bbc-news-summary', split='train')\nfull_dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\ndataset_train = full_dataset['train']\ndataset_valid = full_dataset['test']\n \nprint(dataset_train)\nprint(dataset_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:00:08.948192Z","iopub.execute_input":"2025-03-26T19:00:08.948528Z","iopub.status.idle":"2025-03-26T19:00:35.970839Z","shell.execute_reply.started":"2025-03-26T19:00:08.948500Z","shell.execute_reply":"2025-03-26T19:00:35.970115Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c83369ab9dd4a24912a6826bb05432a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bbc-news-summary.csv:   0%|          | 0.00/7.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"929472a62142421a987f67f9f47cd280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b06e7eda368b4e24a383b3adb3978095"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['File_path', 'Articles', 'Summaries'],\n    num_rows: 1779\n})\nDataset({\n    features: ['File_path', 'Articles', 'Summaries'],\n    num_rows: 445\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Select an article (e.g., the first one)\nsample_article = dataset_train[0]['Articles']\nsample_summary = dataset_train[0]['Summaries']\n\n# Print length of article and summary\nprint(f\"Article Length: {len(sample_article)} characters\")\nprint(f\"Article: {sample_article}\\n\")\nprint(f\"Summary: {sample_summary}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:47:35.989809Z","iopub.execute_input":"2025-03-26T20:47:35.990112Z","iopub.status.idle":"2025-03-26T20:47:35.996483Z","shell.execute_reply.started":"2025-03-26T20:47:35.990089Z","shell.execute_reply":"2025-03-26T20:47:35.995808Z"}},"outputs":[{"name":"stdout","text":"Article Length: 2777 characters\nArticle: Web helps collect aid donations..The web is helping aid agencies gather resources to help cope with the aftermath of the tsunami disaster...Many people are making donations via websites or going online to see how they can get involved with aid efforts. High-profile web portals such as Google, Yahoo, Ebay and Amazon are gathering links that lead people to aid and relief organisations. So many were visiting some aid-related sites that some webpages were struggling to cope with the traffic. An umbrella organisation called the Disasters Emergency Committee (DEC) has been set up by a coalition of 12 charities and has been taking many donations via its specially created website. It urged people to go online where possible to help because donations could be processed more quickly than cash donated in other ways, meaning aid could be delivered as quickly as possible. The site has so far received almost £8 million, with more than 11,000 donations being made online every hour...Telco BT stepped in to take over the secure payments on the DEC site and provided extra logistical support for phone and online appeals after it was initially crippled with online donations. It has also provided space in London's BT tower for one of the call centres dealing with donations...Some of the web's biggest firms are also helping to channel help by modifying their homepages to include links to aid agencies and organisations collecting resources. On its famously sparse homepage Google has placed a link that leads users to a list of sites where donations can be made. Among the 17 organisations listed are Oxfam, Medecins sans Frontieres (Doctors Without Borders) and Network for Good. Many of the sites that Google lists are also taking online donations. Online retailer Amazon has put a large message on its start page that lets people donate money directly to the American Red Cross that will be used with relief efforts. Auction site eBay is giving a list of sites that people can either donate directly to, divert a portion of their profits from anything they sell on eBay to the listed organisations or simply buy items that direct cash to those in the list. Yahoo is proving links direct to charities for those that want to donate. The Auction Drop website is asking people to donate old digital cameras, computers and other gadgets they no longer want that can be auction to raise cash for the aid effort. Sadly, the outpouring of goodwill has also encouraged some conmen to try to cash in. Anti-fraud organisations are warning about e-mails that are starting to circulate which try to convince people to send money directly to them rather than make donations via aid agencies. Those wanting to give cash were urged to use legitimate websites of charities and aid agencies.\n\nSummary: Many of the sites that Google lists are also taking online donations.Many people are making donations via websites or going online to see how they can get involved with aid efforts.On its famously sparse homepage Google has placed a link that leads users to a list of sites where donations can be made.Auction site eBay is giving a list of sites that people can either donate directly to, divert a portion of their profits from anything they sell on eBay to the listed organisations or simply buy items that direct cash to those in the list.An umbrella organisation called the Disasters Emergency Committee (DEC) has been set up by a coalition of 12 charities and has been taking many donations via its specially created website.The site has so far received almost £8 million, with more than 11,000 donations being made online every hour.High-profile web portals such as Google, Yahoo, Ebay and Amazon are gathering links that lead people to aid and relief organisations.Anti-fraud organisations are warning about e-mails that are starting to circulate which try to convince people to send money directly to them rather than make donations via aid agencies.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"MODEL = 't5-base'\nBATCH_SIZE = 4\nNUM_PROCS = 4\nEPOCHS = 10\nOUT_DIR = 'results_t5base'\nMAX_LENGTH = 512 # Maximum context length to consider while preparing dataset.\ntokenizer = T5Tokenizer.from_pretrained(MODEL)\n \n# Function to convert text data into model inputs and targets\ndef preprocess_function(examples):\n    inputs = [f\"summarize: {article}\" for article in examples['Articles']]\n    model_inputs = tokenizer(\n        inputs,\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding='max_length'\n    )\n \n    # Set up the tokenizer for targets\n    targets = [summary for summary in examples['Summaries']]\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=MAX_LENGTH,\n            truncation=True,\n            padding='max_length'\n        )\n \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n \n# Apply the function to the whole dataset\ntokenized_train = dataset_train.map(\n    preprocess_function,\n    batched=True,\n    num_proc=NUM_PROCS\n)\ntokenized_valid = dataset_valid.map(\n    preprocess_function,\n    batched=True,\n    num_proc=NUM_PROCS\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:00:46.776744Z","iopub.execute_input":"2025-03-26T19:00:46.777413Z","iopub.status.idle":"2025-03-26T19:00:52.380488Z","shell.execute_reply.started":"2025-03-26T19:00:46.777380Z","shell.execute_reply":"2025-03-26T19:00:52.379297Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3de83cc66f4733980097c5c7a35595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18e99c378fd4811904e967a932b08d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532c47fe9c204e50b3cb4109df7f5356"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1779 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9b985a607b4749839643aed6887c0e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/445 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ad08df8dc141c4b02e966a1cff347b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(MODEL)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\nrouge = evaluate.load(\"rouge\")\n \ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n \n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n \n    result = rouge.compute(\n        predictions=decoded_preds,\n        references=decoded_labels,\n        use_stemmer=True,\n        rouge_types=[\n            'rouge1',\n            'rouge2',\n            'rougeL'\n        ]\n    )\n \n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n \n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:02:03.141999Z","iopub.execute_input":"2025-03-26T19:02:03.142348Z","iopub.status.idle":"2025-03-26T19:02:09.232958Z","shell.execute_reply.started":"2025-03-26T19:02:03.142318Z","shell.execute_reply":"2025-03-26T19:02:09.232269Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c721583fdbb344b59ccef241ffe7f60a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1433425dde34ee48f7ba7e2dd5018a3"}},"metadata":{}},{"name":"stdout","text":"222,903,552 total parameters.\n222,903,552 training parameters.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176f88e1bc7c4216b31482f2395d8546"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport pprint\nimport evaluate\nimport numpy as np\n \nfrom transformers import (\n    T5Tokenizer,\n    T5ForConditionalGeneration,\n    TrainingArguments,\n    Trainer\n)\nfrom datasets import load_dataset\ndataset = load_dataset('gopalkalpande/bbc-news-summary', split='train')\nfull_dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\ndataset_train = full_dataset['train']\ndataset_valid = full_dataset['test']\n \nprint(dataset_train)\nprint(dataset_valid)\nMODEL = 't5-base'\nBATCH_SIZE = 4\nNUM_PROCS = 4\nEPOCHS = 10\nOUT_DIR = 'results_t5base'\nMAX_LENGTH = 512 # Maximum context length to consider while preparing dataset.\ntokenizer = T5Tokenizer.from_pretrained(MODEL)\n \n# Function to convert text data into model inputs and targets\ndef preprocess_function(examples):\n    inputs = [f\"summarize: {article}\" for article in examples['Articles']]\n    model_inputs = tokenizer(\n        inputs,\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding='max_length'\n    )\n \n    # Set up the tokenizer for targets\n    targets = [summary for summary in examples['Summaries']]\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=MAX_LENGTH,\n            truncation=True,\n            padding='max_length'\n        )\n \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n \n# Apply the function to the whole dataset\ntokenized_train = dataset_train.map(\n    preprocess_function,\n    batched=True,\n    num_proc=NUM_PROCS\n)\ntokenized_valid = dataset_valid.map(\n    preprocess_function,\n    batched=True,\n    num_proc=NUM_PROCS\n)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\nrouge = evaluate.load(\"rouge\")\n \ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n \n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n \n    result = rouge.compute(\n        predictions=decoded_preds,\n        references=decoded_labels,\n        use_stemmer=True,\n        rouge_types=[\n            'rouge1',\n            'rouge2',\n            'rougeL'\n        ]\n    )\n \n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n \n    return {k: round(v, 4) for k, v in result.items()}\ndef preprocess_logits_for_metrics(logits, labels):\n    \"\"\"\n    Original Trainer may have a memory leak.\n    This is a workaround to avoid storing too many tensors that are not needed.\n    \"\"\"\n    pred_ids = torch.argmax(logits[0], dim=-1)\n    return pred_ids, labels\ntraining_args = TrainingArguments(\n    output_dir=OUT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=OUT_DIR,\n    logging_steps=10,\n    evaluation_strategy='steps',\n    eval_steps=200,\n    save_strategy='epoch',\n    save_total_limit=2,\n    report_to='tensorboard',\n    learning_rate=0.0001,\n    dataloader_num_workers=4\n)\n \ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_valid,\n    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n    compute_metrics=compute_metrics\n)\n \nhistory = trainer.train()\nmodel_path = f\"{OUT_DIR}\"  # the path where you saved your model\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\ntokenizer = T5Tokenizer.from_pretrained(OUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:02:39.180818Z","iopub.execute_input":"2025-03-26T19:02:39.181200Z","iopub.status.idle":"2025-03-26T19:23:51.285960Z","shell.execute_reply.started":"2025-03-26T19:02:39.181174Z","shell.execute_reply":"2025-03-26T19:23:51.285021Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='669' max='669' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [669/669 21:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.326000</td>\n      <td>0.412758</td>\n      <td>0.897200</td>\n      <td>0.823400</td>\n      <td>0.878200</td>\n      <td>238.251700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.396100</td>\n      <td>0.365527</td>\n      <td>0.903500</td>\n      <td>0.833100</td>\n      <td>0.885500</td>\n      <td>238.528100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.388000</td>\n      <td>0.351529</td>\n      <td>0.906900</td>\n      <td>0.837400</td>\n      <td>0.889100</td>\n      <td>238.528100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nprint(OUT_DIR)  # Check what the variable is storing\nprint(os.listdir(OUT_DIR))  # See if tokenizer files exist\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:25:04.604087Z","iopub.execute_input":"2025-03-26T20:25:04.604368Z","iopub.status.idle":"2025-03-26T20:25:04.609327Z","shell.execute_reply.started":"2025-03-26T20:25:04.604346Z","shell.execute_reply":"2025-03-26T20:25:04.608627Z"}},"outputs":[{"name":"stdout","text":"results_t5base\n['checkpoint-446', 'checkpoint-669', 'events.out.tfevents.1743015759.fadca9d4e559.69.0']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import T5Tokenizer\n\n# Load tokenizer from Hugging Face\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n# Save inside results_t5base so you can reuse it\ntokenizer.save_pretrained(\"results_t5base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:26:56.342191Z","iopub.execute_input":"2025-03-26T20:26:56.342470Z","iopub.status.idle":"2025-03-26T20:26:56.731873Z","shell.execute_reply.started":"2025-03-26T20:26:56.342449Z","shell.execute_reply":"2025-03-26T20:26:56.731193Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"('results_t5base/tokenizer_config.json',\n 'results_t5base/special_tokens_map.json',\n 'results_t5base/spiece.model',\n 'results_t5base/added_tokens.json')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(\"results_t5base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:30:05.427726Z","iopub.execute_input":"2025-03-26T20:30:05.428063Z","iopub.status.idle":"2025-03-26T20:30:05.624602Z","shell.execute_reply.started":"2025-03-26T20:30:05.428034Z","shell.execute_reply":"2025-03-26T20:30:05.623617Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"input_text = \"summarize: You're Malak Raaf, a senior Computer Science student at Ain Shams University. You have a strong interest in data science and are currently looking for data engineering opportunities. You’ve worked extensively with optimization problems, particularly related to transportation and logistics, using Excel and Python. Your projects involve data processing, linear programming, and GUI development for data applications. You've also worked with arXiv educational article datasets, indicating an interest in NLP or academic data analysis.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:34:30.268205Z","iopub.execute_input":"2025-03-26T20:34:30.268560Z","iopub.status.idle":"2025-03-26T20:34:30.272109Z","shell.execute_reply.started":"2025-03-26T20:34:30.268533Z","shell.execute_reply":"2025-03-26T20:34:30.271329Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:34:33.013998Z","iopub.execute_input":"2025-03-26T20:34:33.014319Z","iopub.status.idle":"2025-03-26T20:34:33.019406Z","shell.execute_reply.started":"2025-03-26T20:34:33.014293Z","shell.execute_reply":"2025-03-26T20:34:33.018612Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"output_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\nsummary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(\"Summary:\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:34:36.880167Z","iopub.execute_input":"2025-03-26T20:34:36.880457Z","iopub.status.idle":"2025-03-26T20:34:47.908304Z","shell.execute_reply.started":"2025-03-26T20:34:36.880435Z","shell.execute_reply":"2025-03-26T20:34:47.907461Z"}},"outputs":[{"name":"stdout","text":"Summary: You're Malak Raaf, a senior Computer Science student at Ain Shams University. You’ve worked extensively with optimization problems, particularly related to transportation and logistics, using Excel and Python. Your projects involve data processing, linear programming, and GUI development for data applications. You've also worked with arXiv educational article datasets, indicating an interest in NLP or academic data analysis.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"input_text_3 = \"\"\"\nWeb helps collect aid donations..The web is helping aid agencies gather resources to help cope with the aftermath of the tsunami disaster...Many people are making donations via websites or going online to see how they can get involved with aid efforts. High-profile web portals such as Google, Yahoo, Ebay and Amazon are gathering links that lead people to aid and relief organisations. So many were visiting some aid-related sites that some webpages were struggling to cope with the traffic. An umbrella organisation called the Disasters Emergency Committee (DEC) has been set up by a coalition of 12 charities and has been taking many donations via its specially created website. It urged people to go online where possible to help because donations could be processed more quickly than cash donated in other ways, meaning aid could be delivered as quickly as possible. The site has so far received almost £8 million, with more than 11,000 donations being made online every hour...Telco BT stepped in to take over the secure payments on the DEC site and provided extra logistical support for phone and online appeals after it was initially crippled with online donations. It has also provided space in London's BT tower for one of the call centres dealing with donations...Some of the web's biggest firms are also helping to channel help by modifying their homepages to include links to aid agencies and organisations collecting resources. On its famously sparse homepage Google has placed a link that leads users to a list of sites where donations can be made. Among the 17 organisations listed are Oxfam, Medecins sans Frontieres (Doctors Without Borders) and Network for Good. Many of the sites that Google lists are also taking online donations. Online retailer Amazon has put a large message on its start page that lets people donate money directly to the American Red Cross that will be used with relief efforts. Auction site eBay is giving a list of sites that people can either donate directly to, divert a portion of their profits from anything they sell on eBay to the listed organisations or simply buy items that direct cash to those in the list. Yahoo is proving links direct to charities for those that want to donate. The Auction Drop website is asking people to donate old digital cameras, computers and other gadgets they no longer want that can be auction to raise cash for the aid effort. Sadly, the outpouring of goodwill has also encouraged some conmen to try to cash in. Anti-fraud organisations are warning about e-mails that are starting to circulate which try to convince people to send money directly to them rather than make donations via aid agencies. Those wanting to give cash were urged to use legitimate websites of charities and aid agencies.\n\"\"\"\ninput_ids = tokenizer(input_text_3, return_tensors=\"pt\").input_ids\n\noutput_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\nsummary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(\"Summary:\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T20:49:34.754593Z","iopub.execute_input":"2025-03-26T20:49:34.754901Z","iopub.status.idle":"2025-03-26T20:50:00.815261Z","shell.execute_reply.started":"2025-03-26T20:49:34.754880Z","shell.execute_reply":"2025-03-26T20:50:00.814486Z"}},"outputs":[{"name":"stdout","text":"Summary: many of the sites that Google lists are also taking online donations.Auction site eBay is giving a list of sites that people can either donate directly to, divert a portion of their profits from anything they sell on eBay to the listed organisations or simply buy items that direct cash to those in the list.The web is helping aid agencies gather resources to help cope with the aftermath of the tsunami disaster.Auction site eBay is giving a list of sites that people can either donate directly to, divert a portion of their profits from anything they sell on eBay to the listed organisations or simply buy items that direct cash to those in the list.Some of the web's biggest firms are also helping to channel help\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Load an article and its original summary from dataset\nsample_article = dataset_train[0]['Articles']\noriginal_summary = dataset_train[0]['Summaries']\n\n# Print original summary\nprint(f\"Original Summary: {original_summary}\\n\")\n\n# Tokenize the article\ninput_ids = tokenizer(sample_article, return_tensors=\"pt\").input_ids\n\n# Generate summary using T5 model\noutput_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\ngenerated_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n# Print generated summary\nprint(f\"Generated Summary: {generated_summary}\\n\")\n\n# Compare using ROUGE Score\nfrom rouge_score import rouge_scorer\n\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\nscores = scorer.score(original_summary, generated_summary)\n\nprint(\"ROUGE Scores:\", scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:01:14.341232Z","iopub.execute_input":"2025-03-26T21:01:14.341580Z","iopub.status.idle":"2025-03-26T21:01:39.781980Z","shell.execute_reply.started":"2025-03-26T21:01:14.341552Z","shell.execute_reply":"2025-03-26T21:01:39.781248Z"}},"outputs":[{"name":"stdout","text":"Original Summary: Many of the sites that Google lists are also taking online donations.Many people are making donations via websites or going online to see how they can get involved with aid efforts.On its famously sparse homepage Google has placed a link that leads users to a list of sites where donations can be made.Auction site eBay is giving a list of sites that people can either donate directly to, divert a portion of their profits from anything they sell on eBay to the listed organisations or simply buy items that direct cash to those in the list.An umbrella organisation called the Disasters Emergency Committee (DEC) has been set up by a coalition of 12 charities and has been taking many donations via its specially created website.The site has so far received almost £8 million, with more than 11,000 donations being made online every hour.High-profile web portals such as Google, Yahoo, Ebay and Amazon are gathering links that lead people to aid and relief organisations.Anti-fraud organisations are warning about e-mails that are starting to circulate which try to convince people to send money directly to them rather than make donations via aid agencies.\n\nGenerated Summary: many of the sites that Google lists are also taking online donations.Auction site eBay is giving a list of sites that people can either donate directly to, divert a portion of their profits from anything they sell on eBay to the listed organisations or simply buy items that direct cash to those in the list.The web is helping aid agencies gather resources to help cope with the aftermath of the tsunami disaster.Auction site eBay is giving a list of sites that people can either donate directly to, divert a portion of their profits from anything they sell on eBay to the listed organisations or simply buy items that direct cash to those in the list.Some of the web's biggest firms are also helping to channel help\n\nROUGE Scores: {'rouge1': Score(precision=0.6717557251908397, recall=0.43564356435643564, fmeasure=0.5285285285285286), 'rouge2': Score(precision=0.45384615384615384, recall=0.2935323383084577, fmeasure=0.3564954682779456), 'rougeL': Score(precision=0.5343511450381679, recall=0.3465346534653465, fmeasure=0.42042042042042044)}\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:04:39.363239Z","iopub.execute_input":"2025-03-26T21:04:39.363549Z","iopub.status.idle":"2025-03-26T21:04:39.367277Z","shell.execute_reply.started":"2025-03-26T21:04:39.363521Z","shell.execute_reply":"2025-03-26T21:04:39.366469Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def cosine_similarity_text(text1, text2):\n    vectorizer = CountVectorizer().fit_transform([text1, text2])\n    vectors = vectorizer.toarray()\n    return cosine_similarity(vectors)[0, 1] * 100  # Convert to percentage\n\nsimilarity_cosine = cosine_similarity_text(original_summary, generated_summary)\nprint(f\"Cosine Similarity: {similarity_cosine:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:04:41.071730Z","iopub.execute_input":"2025-03-26T21:04:41.072078Z","iopub.status.idle":"2025-03-26T21:04:41.103001Z","shell.execute_reply.started":"2025-03-26T21:04:41.072051Z","shell.execute_reply":"2025-03-26T21:04:41.102286Z"}},"outputs":[{"name":"stdout","text":"Cosine Similarity: 73.75%\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Load an article and its original summary from dataset\nsample_article = dataset_train[2]['Articles']\noriginal_summary = dataset_train[2]['Summaries']\n\n# Print original summary\nprint(f\"Original Summary: {original_summary}\\n\")\n\n# Tokenize the article\ninput_ids = tokenizer(sample_article, return_tensors=\"pt\").input_ids\n\n# Generate summary using T5 model\noutput_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\ngenerated_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n# Print generated summary\nprint(f\"Generated Summary: {generated_summary}\\n\")\n\n# Compare using ROUGE Score\nfrom rouge_score import rouge_scorer\n\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\nscores = scorer.score(original_summary, generated_summary)\n\nprint(\"ROUGE Scores:\", scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:05:28.723341Z","iopub.execute_input":"2025-03-26T21:05:28.723628Z","iopub.status.idle":"2025-03-26T21:05:54.727740Z","shell.execute_reply.started":"2025-03-26T21:05:28.723605Z","shell.execute_reply":"2025-03-26T21:05:54.726939Z"}},"outputs":[{"name":"stdout","text":"Original Summary: \"He had lost three times to Roddick, and this was his day to beat him.\"Some people have said that I am obsessed but I think that it is better this way.Spain's victory was also remarkable for the performance of Rafael Nadal, who beat Roddick in the opening singles.Carlos Moya described Spain's Davis Cup victory as the highlight of his career after he beat Andy Roddick to end the USA's challenge in Seville.Spain's only other Davis Cup title came two years ago in Valencia, when they beat Australia.And Moya, nicknamed Charly, admitted: \"The Davis Cup is my dream and I was a bit nervous at the outset.\"What a great way to finish the year,\" said Nadal afterwards.\"But certainly I think we can put the work in at the appropriate time and play a couple more events and play against these guys who are the best on this stuff,\" said McEnroe.Roddick was left frustrated after losing both his singles on the slow clay of Seville's Olympic Stadium.\"It's just tough because I felt like I was in it the whole time against one of the top three clay-courters in the world,\" said the American.\"I think it will help these guys even on slow hard courts to learn how to mix things up a little bit and to play a little bit smarter and tactically better.\"\n\nGenerated Summary: I was a bit nervous at the outset.\"It's just tough because I felt like I was in it the whole time against one of the top three clay-courters in the world,\" said the American.Carlos Moya described Spain's Davis Cup victory as the highlight of his career after he beat Andy Roddick to end the USA's challenge in Seville.Moya made up for missing Spain's 2000 victory through injury by beating Roddick 6-2 7-6 (7-1) 7-6 (7-5) to give the hosts an unassailable 3-1 lead.And Moya, nicknamed Charly, admitted: \"The Davis Cup is\n\nROUGE Scores: {'rouge1': Score(precision=0.7735849056603774, recall=0.34309623430962344, fmeasure=0.4753623188405798), 'rouge2': Score(precision=0.6761904761904762, recall=0.29831932773109243, fmeasure=0.4139941690962099), 'rougeL': Score(precision=0.44339622641509435, recall=0.19665271966527198, fmeasure=0.27246376811594203)}\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"def cosine_similarity_text(text1, text2):\n    vectorizer = CountVectorizer().fit_transform([text1, text2])\n    vectors = vectorizer.toarray()\n    return cosine_similarity(vectors)[0, 1] * 100  # Convert to percentage\n\nsimilarity_cosine = cosine_similarity_text(original_summary, generated_summary)\nprint(f\"Cosine Similarity: {similarity_cosine:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:05:54.728804Z","iopub.execute_input":"2025-03-26T21:05:54.729033Z","iopub.status.idle":"2025-03-26T21:05:54.735989Z","shell.execute_reply.started":"2025-03-26T21:05:54.729013Z","shell.execute_reply":"2025-03-26T21:05:54.735015Z"}},"outputs":[{"name":"stdout","text":"Cosine Similarity: 80.02%\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"inputt_text = \"\"\"First: Plant Classification  \nData Preparation \n1. Setting Constants\nimage_size: Defines the target size to which images will be resized (224x224 pixels).\nbatch_size: The number of images to process at once during training. (64 batches)\n2. Loading Data\nThis function loads images and their corresponding labels from the specified directory.\n3. Normalizing Images\nThe images are normalized by dividing each pixel's value by 255.0, converting the pixel values from \nthe range [0, 255] to [0, 1]. This step helps improve the convergence during model training.\n4. Splitting the Data into Train and Validation Sets\n• This splits the loaded data into training and validation sets, with 80% of the data used for \ntraining and 20% for validation \n7. Label Encoding\n• The LabelEncoder is used to convert text labels into integer labels \n8. Class Weight Calculation\n• The class weights are computed to handle class imbalance. The compute_class_weight \nfunction calculates weights inversely proportional to class frequencies.\n10. Data Augmentation\nWe apply augmentation only in MobileNet, VGG, AlexNet, as we don't apply augmentation in \nViT. The train_datagen applies several data augmentation techniques to the training images, \nincluding: Random rotation (rotation_range), width/height shifting, shearing, zooming, and flipping \nthe images horizontally. val_datagen doesn't apply any augmentation\nThese augmentations help improve model generalization by providing more varied input data.\n11. Data Generators\ntrain_generator and validation_generator and testing_generator are instances of \nImageDataGenerator that yield batches of images and labels during training and validation, \nrespectively\nVision Transformer (ViT)\nArchitecture\n• Divides input images into fixed-size non-overlapping patches (e.g., 16×1616 \\times \n1616x16).\n• Converts patches into 1D vector embeddings via a linear projection.\n• Adds positional embeddings to preserve spatial relationships.\n• Processes embeddings using Transformer encoder layers (self-attention + feedforward \nnetworks).\n• Uses a learnable [CLS] token for classification.\n• Final output is passed to a classification head for tasks like image classification.\nAdvantages\n• Scalability: Performs better with larger datasets.\n• Global Context: Captures global relationships across the image.\n• Flexibility: Can adapt to multi-modal tasks beyond vision (e.g., vision + text).\n• Reduced Inductive Bias: Learns more adaptively compared to CNNs.\n• Improved Performance: Outperforms CNNs on benchmarks when pre-trained on large \ndatasets.\n• Parallelization: Faster training due to sequence-level parallel processing.\n• Transfer Learning: Pre-trained ViTs generalize well to other tasks.\nChallenges\n• Data Requirements: Needs large-scale datasets for effective training.\n• Computational Cost: High memory and computation demands due to quadratic self attention complexity.\n• Overfitting: Prone to overfitting on smaller datasets.\n• Interpretability: Harder to interpret learned features compared to CNNs.MobileNet\nMobileNet is a lightweight deep learning model designed for mobile and embedded devices, \nprioritizing efficiency and speed. It uses depthwise separable convolutions to reduce the number of \nparameters and computations. This architecture is well-suited for tasks like image classification \nand object detection on resource-constrained devices. Despite its simplicity, it achieves \ncompetitive accuracy compared to larger models.\nArchitecture:\nInput: Images of size 224x224x3 (RGB).\nBase Model:\n• MobileNet (pre-trained on ImageNet, without the top classification layers).\n• Lightweight and efficient architecture, designed with depthwise separable convolutions for \nreduced computational complexity.\n• Base model layers are frozen (not trainable).\nCustom Layers:\n• Global Average Pooling (GAP): Reduces the spatial dimensions of the feature maps to a \nsingle vector for each channel, summarizing the spatial information globally.\n• Dense Layer: Fully connected layer with 1024 units and ReLU activation.\n• Dropout Layer: Dropout with a rate of 0.5 to reduce overfitting.\n• Output Layer: Dense layer with num_classes units and softmax activation for classification.\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\nOutput: Class probabilities for the given number of output classes \n Best for Resource-Constrained Devices: MobileNet\n• Why: MobileNet is optimized for efficiency and speed, making it ideal for mobile and \nembedded devices. Despite its smaller size, it delivers competitive performance on tasks \nlike image classification and object detection.VGG16\nVGG is a deep convolutional neural network known for its simplicity and uniform architecture, \nconsisting of sequential 3x3 convolutional layers followed by fully connected layers. It comes in \nvariations like VGG-16 and VGG-19, named for the number of layers. VGG models are \ncomputationally expensive but deliver high accuracy in image classification. Their deep and \nuniform structure has influenced the design of many subsequent models.\nArchitecture:\nInput:\n• Images of size 224x224x3 (RGB).\nBase Model:\n• VGG16 (pre-trained on ImageNet, without the top classification layers).\n• Contains 13 convolutional layers grouped into 5 blocks, each followed by max-pooling \nlayers for feature extraction.\nCustom Layers:\n• Flatten: Converts feature maps from VGG16 into a 1D vector.\n• Dense Layer 1: Fully connected layer with 4096 units and ReLU activation.\n• Dropout Layer 1: Dropout with a rate of 0.5 to reduce overfitting.\n• Dense Layer 2: Fully connected layer with 4096 units and ReLU activation.\n• Dropout Layer 2: Another dropout with a rate of 0.5.\n• Output Layer: Dense layer with num_classes units and softmax activation for classification.\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\n Output: Class probabilities for the given number of output classes \nBest for High Accuracy on Large Datasets: VGG\n• Why: VGG models, particularly VGG-16 and VGG-19, provide high accuracy due to their \ndeeper architecture and consistent design. They are well-suited for applications requiring \nprecise feature extraction.\nU-Net Model\nU-Net is a convolutional neural network architecture specifically designed for biomedical image \nsegmentation. It has a symmetrical encoder-decoder structure, where the encoder extracts \nfeatures, and the decoder reconstructs the image with segmentation masks. Skip connections link \ncorresponding layers in the encoder and decoder to preserve spatial information. U-Net is highly \nefficient and performs well on small datasets, making it a popular choice in medical imaging tasks.\nArchitecture\nDefine U-Net Blocks:\n• Implemented a convolutional block (conv_block) that includes two convolutional layers \nwith ReLU activation, kernel initialization, and dropout for regularization.\n• Created an upsampling block (upsample_block) using transposed convolution for \nupsampling and concatenation of features from previous layers.\nContracting Path:\n• Used sequential convolutional blocks (conv_block) and max-pooling layers to reduce \nspatial dimensions while increasing the number of feature channels:\n• Encoder: Extracts and compresses features from the input (downsampling).\nExpanding Path:\n• Applied upsampling blocks to reconstruct spatial dimensions and combine features from \nthe contracting path:\n• Decoder: Reconstructs the spatial dimensions and combines extracted features \n• These stages are connected by the bottleneck layer (c5), which acts as the transition point \nbetween the encoder and decoder.\nOutput Layer:\n• Added a final convolutional layer with 1 filter and sigmoid activation to produce a \nprobability map for binary segmentation.\n• Model Training:\n• Defined callbacks for early stopping and saving the best model:\no EarlyStopping monitored validation loss with a patience of 5 epochs.\no ModelCheckpoint saved the best model during training.\nModel Saving:\n• Saved the trained model in HDF5 format (model.h5).\nSAM Model\nSAM is based on a foundation of transformer models, leveraging the power of attention \nmechanisms to learn spatial relationships within images for precise segmentation. SAM uses a \nvision transformer (ViT) as its backbone. Vision transformers have self-attention mechanisms that \nallow the model to capture long-range dependencies between pixels.\nArchitecture:\nThe main parts of the SAM architecture include:\n• Backbone (Vision Transformer - ViT): This is the core architecture of SAM, where image \nfeatures are extracted.\n• Prompt Encoder: This component processes the different types of input prompts (points, \nboxes, and masks) to guide the segmentation.\n• Segmentation Decoder: This part decodes the model’s predictions into final segmentation \nmasks.\nDice Loss Advantages:\n• Handling Imbalanced Data: Dice Loss is particularly useful when the dataset is \nimbalanced.\nSecond: Plant Disease Recognition\nSiamese Architecture: A neural network designed to determine the similarity or dissimilarity \nbetween two inputs.\nTwin Networks: Consists of two identical sub-networks that share the same weights and \nparameters.\nShared Weights: Both sub-networks learn the same features from the input data, ensuring \nconsistent comparisons.\nDistance Metric: Outputs (feature vectors) from the sub-networks are compared using a \ndistance metric like Euclidean distance or cosine similarity.\nTraining: Network is trained with pairs of images labeled as similar or dissimilar, adjusting \nparameters to bring similar images closer and dissimilar ones farther apart.\nApplication: Commonly used in tasks such as plant recognition or image matching where \npairwise comparisons are necessary\nAdvantages of One-shot Learning in Plant Recognition:\n• Reduced Data Requirements: Recognizes plant species with just one image per species, \nreducing the need for large labeled datasets.\n• Generalization: Effectively generalizes to new, unseen plant species, especially with \nmodels like Siamese or Prototypical Networks.\nAlexNet\nAlexNet is a pioneering deep learning model that popularized convolutional neural networks in the \n2012 ImageNet competition. It uses five convolutional layers, followed by three fully connected \nlayers, and employs techniques like ReLU activation, dropout, and data augmentation. AlexNet \nsignificantly reduced error rates at the time and laid the foundation for modern deep learning in \ncomputer vision.\nArchitecture:\nInput:\n• Accepts images of size 224x224x3 (RGB).\nFeature Extraction (Convolutional and Pooling Layers):\n• 5 convolutional layers: filters with ReLU activation.Followed by MaxPooling\nFlatten and Dense Layers:\n• Flatten: Converts the extracted features into a 1D vector.\n• Dense Layer 1 & Dense Layer 2: 4096 units, with ReLU activation.Followed by Dropout (rate \n0.5) to reduce overfitting.\nOutput Layer: A dense layer with num_classes units and softmax activation\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\nOutput: Produces class probabilities for classification tasks \nWorst Model: Context Matters\no Why: While AlexNet was groundbreaking in 2012, its architecture is now \nconsidered outdated compared to more efficient and deeper models like VGG and \nMobileNet. It has fewer layers, lower accuracy, and lacks optimizations like \ndepthwise separable convolutions.\no Drawback: Inefficiencies and limitations make it less competitive in scenarios \nwhere computational resources and accuracy are critical\n\n\"\"\"\ninput_ids = tokenizer(inputt_text, return_tensors=\"pt\").input_ids\n\noutput_ids = model.generate(input_ids, max_length=500, num_beams=4, early_stopping=True)\nsummary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(\"Summary:\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:10:47.630065Z","iopub.execute_input":"2025-03-26T21:10:47.630372Z","iopub.status.idle":"2025-03-26T21:12:00.023979Z","shell.execute_reply.started":"2025-03-26T21:10:47.630348Z","shell.execute_reply":"2025-03-26T21:12:00.023018Z"}},"outputs":[{"name":"stdout","text":"Summary: a pixel value by 255.0, converting the pixel values from the range [0, 1] to [0, 1]. Output: Class probabilities for the given number of output classes Best for Resource-Constrained Devices: VGG • Why: VGG is a lightweight deep learning model designed for mobile and embedded devices, prioritizing efficiency and speed. Base Model: • VGG16 (pre-trained on ImageNet, without the top classification layers). Output Layer: A dense layer with num_classes units and softmax activation.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"print(tokenizer.model_max_length)  # Shows max tokens supported\n\n1000000000000000019884624838656","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:13:18.371487Z","iopub.execute_input":"2025-03-26T21:13:18.371880Z","iopub.status.idle":"2025-03-26T21:13:18.376474Z","shell.execute_reply.started":"2025-03-26T21:13:18.371850Z","shell.execute_reply":"2025-03-26T21:13:18.375621Z"}},"outputs":[{"name":"stdout","text":"1000000000000000019884624838656\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"print(tokenizer.model_max_length)\nprint(model.config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:14:29.962089Z","iopub.execute_input":"2025-03-26T21:14:29.962397Z","iopub.status.idle":"2025-03-26T21:14:29.968309Z","shell.execute_reply.started":"2025-03-26T21:14:29.962375Z","shell.execute_reply":"2025-03-26T21:14:29.967369Z"}},"outputs":[{"name":"stdout","text":"1000000000000000019884624838656\nT5Config {\n  \"_attn_implementation_autoset\": true,\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 3072,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.50.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the model and tokenizer\nmodel_name = \"t5-small\"  # You can also try \"t5-base\" or \"t5-large\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Function to split text into chunks of 512 tokens\ndef chunk_text(text, tokenizer, chunk_size=512):\n    tokens = tokenizer.encode(text, truncation=False)  # Tokenize without truncation\n    return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n\n# Function to summarize each chunk and combine them\ndef summarize_long_text(text, tokenizer, model):\n    chunks = chunk_text(text, tokenizer)\n\n    summaries = []\n    for chunk in chunks:\n        input_ids = torch.tensor([chunk])  # Convert chunk to tensor\n        output_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        summaries.append(summary)\n\n    final_summary = \" \".join(summaries)  # Merge all summaries\n    return final_summary\n\n# Example long text (Replace with your actual text)\nlong_text =  \"\"\"First: Plant Classification  \nData Preparation \n1. Setting Constants\nimage_size: Defines the target size to which images will be resized (224x224 pixels).\nbatch_size: The number of images to process at once during training. (64 batches)\n2. Loading Data\nThis function loads images and their corresponding labels from the specified directory.\n3. Normalizing Images\nThe images are normalized by dividing each pixel's value by 255.0, converting the pixel values from \nthe range [0, 255] to [0, 1]. This step helps improve the convergence during model training.\n4. Splitting the Data into Train and Validation Sets\n• This splits the loaded data into training and validation sets, with 80% of the data used for \ntraining and 20% for validation \n7. Label Encoding\n• The LabelEncoder is used to convert text labels into integer labels \n8. Class Weight Calculation\n• The class weights are computed to handle class imbalance. The compute_class_weight \nfunction calculates weights inversely proportional to class frequencies.\n10. Data Augmentation\nWe apply augmentation only in MobileNet, VGG, AlexNet, as we don't apply augmentation in \nViT. The train_datagen applies several data augmentation techniques to the training images, \nincluding: Random rotation (rotation_range), width/height shifting, shearing, zooming, and flipping \nthe images horizontally. val_datagen doesn't apply any augmentation\nThese augmentations help improve model generalization by providing more varied input data.\n11. Data Generators\ntrain_generator and validation_generator and testing_generator are instances of \nImageDataGenerator that yield batches of images and labels during training and validation, \nrespectively\nVision Transformer (ViT)\nArchitecture\n• Divides input images into fixed-size non-overlapping patches (e.g., 16×1616 \\times \n1616x16).\n• Converts patches into 1D vector embeddings via a linear projection.\n• Adds positional embeddings to preserve spatial relationships.\n• Processes embeddings using Transformer encoder layers (self-attention + feedforward \nnetworks).\n• Uses a learnable [CLS] token for classification.\n• Final output is passed to a classification head for tasks like image classification.\nAdvantages\n• Scalability: Performs better with larger datasets.\n• Global Context: Captures global relationships across the image.\n• Flexibility: Can adapt to multi-modal tasks beyond vision (e.g., vision + text).\n• Reduced Inductive Bias: Learns more adaptively compared to CNNs.\n• Improved Performance: Outperforms CNNs on benchmarks when pre-trained on large \ndatasets.\n• Parallelization: Faster training due to sequence-level parallel processing.\n• Transfer Learning: Pre-trained ViTs generalize well to other tasks.\nChallenges\n• Data Requirements: Needs large-scale datasets for effective training.\n• Computational Cost: High memory and computation demands due to quadratic self attention complexity.\n• Overfitting: Prone to overfitting on smaller datasets.\n• Interpretability: Harder to interpret learned features compared to CNNs.MobileNet\nMobileNet is a lightweight deep learning model designed for mobile and embedded devices, \nprioritizing efficiency and speed. It uses depthwise separable convolutions to reduce the number of \nparameters and computations. This architecture is well-suited for tasks like image classification \nand object detection on resource-constrained devices. Despite its simplicity, it achieves \ncompetitive accuracy compared to larger models.\nArchitecture:\nInput: Images of size 224x224x3 (RGB).\nBase Model:\n• MobileNet (pre-trained on ImageNet, without the top classification layers).\n• Lightweight and efficient architecture, designed with depthwise separable convolutions for \nreduced computational complexity.\n• Base model layers are frozen (not trainable).\nCustom Layers:\n• Global Average Pooling (GAP): Reduces the spatial dimensions of the feature maps to a \nsingle vector for each channel, summarizing the spatial information globally.\n• Dense Layer: Fully connected layer with 1024 units and ReLU activation.\n• Dropout Layer: Dropout with a rate of 0.5 to reduce overfitting.\n• Output Layer: Dense layer with num_classes units and softmax activation for classification.\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\nOutput: Class probabilities for the given number of output classes \n Best for Resource-Constrained Devices: MobileNet\n• Why: MobileNet is optimized for efficiency and speed, making it ideal for mobile and \nembedded devices. Despite its smaller size, it delivers competitive performance on tasks \nlike image classification and object detection.VGG16\nVGG is a deep convolutional neural network known for its simplicity and uniform architecture, \nconsisting of sequential 3x3 convolutional layers followed by fully connected layers. It comes in \nvariations like VGG-16 and VGG-19, named for the number of layers. VGG models are \ncomputationally expensive but deliver high accuracy in image classification. Their deep and \nuniform structure has influenced the design of many subsequent models.\nArchitecture:\nInput:\n• Images of size 224x224x3 (RGB).\nBase Model:\n• VGG16 (pre-trained on ImageNet, without the top classification layers).\n• Contains 13 convolutional layers grouped into 5 blocks, each followed by max-pooling \nlayers for feature extraction.\nCustom Layers:\n• Flatten: Converts feature maps from VGG16 into a 1D vector.\n• Dense Layer 1: Fully connected layer with 4096 units and ReLU activation.\n• Dropout Layer 1: Dropout with a rate of 0.5 to reduce overfitting.\n• Dense Layer 2: Fully connected layer with 4096 units and ReLU activation.\n• Dropout Layer 2: Another dropout with a rate of 0.5.\n• Output Layer: Dense layer with num_classes units and softmax activation for classification.\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\n Output: Class probabilities for the given number of output classes \nBest for High Accuracy on Large Datasets: VGG\n• Why: VGG models, particularly VGG-16 and VGG-19, provide high accuracy due to their \ndeeper architecture and consistent design. They are well-suited for applications requiring \nprecise feature extraction.\nU-Net Model\nU-Net is a convolutional neural network architecture specifically designed for biomedical image \nsegmentation. It has a symmetrical encoder-decoder structure, where the encoder extracts \nfeatures, and the decoder reconstructs the image with segmentation masks. Skip connections link \ncorresponding layers in the encoder and decoder to preserve spatial information. U-Net is highly \nefficient and performs well on small datasets, making it a popular choice in medical imaging tasks.\nArchitecture\nDefine U-Net Blocks:\n• Implemented a convolutional block (conv_block) that includes two convolutional layers \nwith ReLU activation, kernel initialization, and dropout for regularization.\n• Created an upsampling block (upsample_block) using transposed convolution for \nupsampling and concatenation of features from previous layers.\nContracting Path:\n• Used sequential convolutional blocks (conv_block) and max-pooling layers to reduce \nspatial dimensions while increasing the number of feature channels:\n• Encoder: Extracts and compresses features from the input (downsampling).\nExpanding Path:\n• Applied upsampling blocks to reconstruct spatial dimensions and combine features from \nthe contracting path:\n• Decoder: Reconstructs the spatial dimensions and combines extracted features \n• These stages are connected by the bottleneck layer (c5), which acts as the transition point \nbetween the encoder and decoder.\nOutput Layer:\n• Added a final convolutional layer with 1 filter and sigmoid activation to produce a \nprobability map for binary segmentation.\n• Model Training:\n• Defined callbacks for early stopping and saving the best model:\no EarlyStopping monitored validation loss with a patience of 5 epochs.\no ModelCheckpoint saved the best model during training.\nModel Saving:\n• Saved the trained model in HDF5 format (model.h5).\nSAM Model\nSAM is based on a foundation of transformer models, leveraging the power of attention \nmechanisms to learn spatial relationships within images for precise segmentation. SAM uses a \nvision transformer (ViT) as its backbone. Vision transformers have self-attention mechanisms that \nallow the model to capture long-range dependencies between pixels.\nArchitecture:\nThe main parts of the SAM architecture include:\n• Backbone (Vision Transformer - ViT): This is the core architecture of SAM, where image \nfeatures are extracted.\n• Prompt Encoder: This component processes the different types of input prompts (points, \nboxes, and masks) to guide the segmentation.\n• Segmentation Decoder: This part decodes the model’s predictions into final segmentation \nmasks.\nDice Loss Advantages:\n• Handling Imbalanced Data: Dice Loss is particularly useful when the dataset is \nimbalanced.\nSecond: Plant Disease Recognition\nSiamese Architecture: A neural network designed to determine the similarity or dissimilarity \nbetween two inputs.\nTwin Networks: Consists of two identical sub-networks that share the same weights and \nparameters.\nShared Weights: Both sub-networks learn the same features from the input data, ensuring \nconsistent comparisons.\nDistance Metric: Outputs (feature vectors) from the sub-networks are compared using a \ndistance metric like Euclidean distance or cosine similarity.\nTraining: Network is trained with pairs of images labeled as similar or dissimilar, adjusting \nparameters to bring similar images closer and dissimilar ones farther apart.\nApplication: Commonly used in tasks such as plant recognition or image matching where \npairwise comparisons are necessary\nAdvantages of One-shot Learning in Plant Recognition:\n• Reduced Data Requirements: Recognizes plant species with just one image per species, \nreducing the need for large labeled datasets.\n• Generalization: Effectively generalizes to new, unseen plant species, especially with \nmodels like Siamese or Prototypical Networks.\nAlexNet\nAlexNet is a pioneering deep learning model that popularized convolutional neural networks in the \n2012 ImageNet competition. It uses five convolutional layers, followed by three fully connected \nlayers, and employs techniques like ReLU activation, dropout, and data augmentation. AlexNet \nsignificantly reduced error rates at the time and laid the foundation for modern deep learning in \ncomputer vision.\nArchitecture:\nInput:\n• Accepts images of size 224x224x3 (RGB).\nFeature Extraction (Convolutional and Pooling Layers):\n• 5 convolutional layers: filters with ReLU activation.Followed by MaxPooling\nFlatten and Dense Layers:\n• Flatten: Converts the extracted features into a 1D vector.\n• Dense Layer 1 & Dense Layer 2: 4096 units, with ReLU activation.Followed by Dropout (rate \n0.5) to reduce overfitting.\nOutput Layer: A dense layer with num_classes units and softmax activation\nOptimization:\n• Uses Adam optimizer, categorical cross-entropy loss, and accuracy as a performance \nmetric.\nOutput: Produces class probabilities for classification tasks \nWorst Model: Context Matters\no Why: While AlexNet was groundbreaking in 2012, its architecture is now \nconsidered outdated compared to more efficient and deeper models like VGG and \nMobileNet. It has fewer layers, lower accuracy, and lacks optimizations like \ndepthwise separable convolutions.\no Drawback: Inefficiencies and limitations make it less competitive in scenarios \nwhere computational resources and accuracy are critical\n\n\"\"\"\n\n# Get final summary\nsummary = summarize_long_text(long_text, tokenizer, model)\n\nprint(\"Final Summary:\")\nprint(summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:16:36.970156Z","iopub.execute_input":"2025-03-26T21:16:36.970652Z","iopub.status.idle":"2025-03-26T21:17:18.272400Z","shell.execute_reply.started":"2025-03-26T21:16:36.970608Z","shell.execute_reply":"2025-03-26T21:17:18.271320Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c40dee943e94089b8fabd3e0c71d571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3fd2feb4b314177bd9bd765f98e7091"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc87a53fc4349c8a35a00d8276cf88d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5ba23d8239443a4a6277d0d919bae8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64b7466408fe49049173995d40f9fa10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6059cefe1f546abb38120028129d94c"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2665 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Final Summary:\ntraining and validation, with 80% of the data used for training and validation. 20% for validation 7. Label Encoding • The LabelEncoder is used to convert text labels into integer labels 8. Class Weight Calculation • The class weights are computed to handle class imbalance. Advantages • Scalability: Performs better with larger datasets. . • Parallelization: Faster training due to sequence-level parallel processing. • Parallelization: Faster training due to sequence-level parallel processing. • Parallelization: Faster training due to sequence-level parallel processing. • Parallelization: Faster training due to sequence-level parallel processing. • Parallelization: Faster training due to sequence-level parallel processing. • Transfer Learning: Can adapt to multi-modal tasks beyond vision (e.g., vision + text). • Parallelization: Faster training due to sequence-level parallel layer extraction. • Output Layer 1: Dropout with a rate of 0.5 to reduce overfitting. • Output Layer 1: Dropout with a rate of 0.5. • Output Layer: Dense layer with 4096 units and ReLU activation. • Dropout Layer 2: Dropout with a rate of 0.5. • Output Layer: Dense layer with 4096 units and ReLU activation. • Dropout Layer 2: Dropout with a rate of 0.5. sigmoid activation to produce a probability map for binary segmentation. Model Training: • Defined callbacks for early stopping and saving the best model. • Defined callbacks for early stopping and saving the best model: o EarlyStopping monitored validation loss with a patience of 5 epochs. • Defined callbacks for early stopping and saving the best model: o EarlyStopping monitored validation loss with a patience of 5 epochs Layers: • 5 convolutional layers: filters with ReLU activation.Followed by MaxPooling Flatten and Dense Layers: • Flatten: Converts the extracted features into a 1D vector. Distance Metric: Outputs (feature vectors) from the sub-networks learn the same features from the input data, ensuring consistent comparisons. Distance Metric: Outputs (feature vectors) from the sub-networks learn the same features from the . Output: Produces class probabilities for classification tasks Worst Model: Context Matters o Why: While AlexNet was groundbreaking in 2012, its architecture is now considered outdated compared to more efficient and deeper models like VGG and MobileNet. Inefficiencies and limitations make it less competitive in scenarios where computational resources and accuracy are critical.\n","output_type":"stream"}],"execution_count":45}]}